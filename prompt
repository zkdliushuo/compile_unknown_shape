\section{Evaluation}

\label{sec:eval}



Our evaluation mainly focuses on the following questions:

\begin{itemize}

    \item Can \mysys enhance performance across a diverse set of operators and neural networks on Ascend devices?

    \item Can \mysys effectively infer more invariable parameters for kernels with a reasonable compilation overhead compared to current compilers given same context information?

    \item Can \mysys be useful for accelerators beyond Ascend which have high scalar capablity, such as GPUs?

\end{itemize}

Apart from the above main questions, we evaluate the detailed performance metrics differnce brought by \mysys, the performance improvement under different shape configurations.



\subsection{Experimental Setup}

\label{sec:eval:setup}



\paragraph{Hardware and Software Platforms}

The main evaluation is on a platform equipped with an Ascend 910B1 NPU with 64GB HBM, an Intel Core i7-8700 CPU and 64GB DRAM.

The version of Ascend Compute Architecture for Neural Networks(CANN) is 8.0.RC1.

The version of LLVM is 15.0.5 and the version of PyTorch is 2.6.0.

To ensure accuracy, the reported data represents the average of the last 90 runs out of 100 total executions.



\paragraph{Benchmarks}

\label{sec:eval:bench}

To ensure a comprehensive evaluation, our benchmark suite comprises two main components: individual operator latency and end-to-end model inference.



The operator benchmark is derived from mainstream deep learning workloads, encompassing Large Language Models, Computer Vision models, and recommendation models.

\autoref{tab:dynamic_operators} summarizes the categorization and configurations of the evaluated operators.



\input{tables/operator-specs.tex}





Within this set, we particularly emphasize on complex fused operators widely adopted in industrial serving scenarios, such as \texttt{FlashAttention}, \texttt{FeedForward} (FFN), and \texttt{BatchMatMul}.

These operators are characterized by complex implementations and a substantial number of kernel arguments, making them the primary targets for our optimization.

For the end-to-end model evaluation, we select Qwen2-1.5B and Llama2-7B to represent varying scales of modern LLMs.

We assume batch size of LLMs is fixed and the sequence length of the input prompt and kv cache is variable.



\paragraph{Implementation and Experimental Setup}

All evaluations for both operators and models are conducted using PyTorch 2.6.0.

We encapsulate the target operators and models as \texttt{nn.Module} instances.

Dynamic dimensions are explicitly marked using the \texttt{Dynamic} API, and we utilize PyTorch Dynamo with dynamic compilation enabled.



For the operator benchmark, we compare \textbf{\mysys} against the following methods:

\begin{itemize}

    \item \textbf{Baseline:} The standard offline compilation approach. It compiles the operator library based on standard LLVM passes without access to any runtime operator invocation context.

    \item \textbf{LLVM-spec:} A specialization method relying on standard LLVM capabilities. Leveraging the host-side entry function context provided by \mysys, we identify LLVM IR instructions that access constant function arguments and replace them with their corresponding constant values. We then enable the standard LLVM pass pipeline related to constant optimization.

    \item \textbf{Optimal:} An oracle approach serving as the performance upper bound. For each specific input shape, we first execute the operator to capture the actual runtime values of the scalar arguments passed to the kernel. We then perform constant substitution on the kernel's LLVM IR using these captured values and apply the kernel specialization module of \mysys.

\end{itemize}



\begin{figure}[htbp]

    \centering

    \includegraphics[width=\textwidth]{figures/evaluation/op-speedup/speedup_histogram.pdf}

    \caption{Overall normalized performance of \mysys compared to compiler baselines }

    \label{fig:eval:op-overall}

\end{figure}



\subsection{Performance Result of Operators and Models}

\label{sec:eval:result}



\paragraph{Operator Performance Analysis}

\autoref{fig:eval:op-overall} presents the normalized speedup of the evaluated operators across three methods: \textbf{LLVM-spec}, \textbf{\mysys}, and the \textbf{Optimal} upper bound. All results are normalized to the \textbf{Baseline} (standard offline compilation).



Overall, \textbf{\mysys} achieves a geometric mean speedup of \textbf{1.05$\times$} across all operators, significantly outperforming the \textbf{LLVM-spec} baseline (1.01$\times$). Crucially, the performance of \textbf{\mysys} is nearly identical to the \textbf{Optimal} oracle (within 0.5\% difference on average), demonstrating that our framework effectively captures and leverages almost all available runtime constant information for specialization without the heavy overhead of full compilation for each possible shape.



We observe distinct performance behaviors across different operator categories defined in \autoref{tab:dynamic_operators}:



\begin{itemize}

    \item \textbf{Attention mechanisms (P-Attn, D-Attn):} \mysys achieves the most significant gains in attention-related operators, with speedups reaching \textbf{1.15$\times$} for Prefill FlashAttention (P-Attn) and \textbf{1.18$\times$} for Decoding FlashAttention (\texttt{D-Attn}). 

    These kernels heavily rely on tiling parameters (e.g., block sizes derived from sequence lengths).

    While \textbf{LLVM-spec} fails to optimize these due to the complexity of index calculations, \mysys successfully propagates these runtime scalars into the kernel IR, enabling the backend compiler to generate more efficient pipelined instructions.

    

    \item \textbf{Normalization and Up/Down sampling (GN, RMS, Resize):} Computation-intensive element-wise or reduction ops like GroupNorm (GN) and Resize also benefit notably, showing speedups of approximately \textbf{1.11$\times$} and \textbf{1.10$\times$} respectively. The constant specialization allows for the elimination of redundant boundary checks and simplifies division operations related to channel or spatial dimensions.

    

    % \item \textbf{Performance degradation:} As shown in the breakdown (e.g., for \textit{Pool3D} and \textit{P-MM}), \textbf{LLVM-spec} sometimes yields negligible improvements or even slight regressions (speedup $<$ 1.0). This indicates that merely replacing constant arguments without analyzing their propagation paths (as \mysys does) is insufficient to trigger high-level compiler optimizations on the Ascend NPU.

\end{itemize}



\paragraph{End-to-End Model Performance Analysis}

We further evaluate how the operator-level speedups translate to end-to-end model inference performance. \autoref{fig:eval:model-overall} depicts the normalized speedup for \textbf{Llama2-7B} and \textbf{Qwen2-1.5B} across varying sequence lengths and batch sizes (BS=1, 2, 4) during both Prefill and Decode phases.



\textbf{\mysys} consistently improves inference latency across all configurations, with speedups ranging from \textbf{1.01$\times$ to 1.04$\times$}.

While the end-to-end gains are naturally diluted compared to individual kernel speedups (due to non-computational overheads such as kernel launching, memory copying, and unoptimized operators), the improvements remain robust.



Key observations from the model benchmarks include:

\begin{itemize}

    \item \textbf{Consistency across Phases:} Performance gains are observed in both the compute-bound Prefill phase and the memory-bound Decode phase. This confirms that \mysys effectively optimizes both large-GEMM style workloads (via P-Attn and FFN optimizations) and memory-intensive workloads (via \texttt{D-Attn} and Norm optimizations).

    \item \textbf{Impact of Batch Size and Number of Tokens:} As the batch size increases from 1 to 4, the speedup remains stable. In the Decode phase for Llama2-7B (red line), we observe a slight increase in speedup at certain sequence lengths. This suggests that as the computational intensity increases with larger batches, the benefits of optimized tiling and loop structures generated by \mysys become more pronounced compared to the runtime scheduling overhead.

    

    % \item \textbf{Model Sensitivity:} Qwen2-1.5B generally exhibits similar speedup trends to Llama2-7B, indicating that \mysys is generalizable across different model architectures (e.g., different configurations of GQA or hidden dimensions) without requiring manual tuning.

\end{itemize}



% In summary, \mysys successfully bridges the gap between static compilation and runtime-adaptive optimization, delivering "Optimal-like" performance with negligible overhead.



\begin{figure}

    \centering

    \includegraphics[width=\textwidth]{figures/evaluation/model-speedup/multi_model_speedup.png}

    \caption{Overall normalized performance of \mysys compared to PyTorch baseline based on CANN compiler across various dynamic shape operators. For prefill phase, we measure the time to first token (TTFT); for decode phase, we measure the average time per output token(TPOT).}

    \Description{Overall normalized performance of \mysys compared to PyTorch baseline based on CANN compiler across various dynamic shape operators.}

    \label{fig:eval:model-overall}

\end{figure}



\paragraph{Comparison between \mysys and specialization based on Standard LLVM}



We analyze the trade-off between compilation overhead and optimization effects, as detailed in \autoref{tab:compilation_overhead_final}.



LLVM-spec incurs minimal compilation overhead relative to the baseline (less than 0.60s in all cases).

This efficiency stems from its efficient implementation: 

it scans both host and device-side programs to perform immediate constant substitution on IR instructions that access constant arguments, followed by a cascade of standard constant-related optimization passes.

Crucially, these standard passes are predominantly \textit{context-insensitive} and \textit{flow-insensitive}, allowing them to complete the analysis rapidly without deeply tracking complex variable states along the control flow and the function call graph.



In contrast, \mysys exhibits a moderate increase in compilation time, ranging from 0.24s to 2.70s.

This additional overhead is inherently driven by the adoption of \textit{flow-sensitive} and \textit{context-sensitive} analysis.

Unlike the direct substitution in LLVM-spec, \mysys must infer and propagate significantly more detailed information throughout the kernel's execution path.

However, given that this compilation is a one-off cost during deployment, we consider this overhead acceptable, especially as it enables the backend compiler to generate significantly more efficient binaries (as evidenced by the 1.05$\times$ speedup).

Besides, \mysys avoids the need for path-sensitive analysis due to early-return paths pruning, makes the compilation overhead acceptable.



% TODO:

% Comparison of the number of inferred parameters



\input{tables/compilation_overhead.tex}



\subsection{Analysis on Benefits of Kernel Specialization}



\input{tables/benefits-all}



\label{sec:eval:analysis}



To deeply understand the sources of the performance gains demonstrated in \autoref{sec:eval:result}, we analyze low-level hardware performance metrics collected during operator execution.

\autoref{tab:performance_metrics} details the Speedup alongside key architectural metrics: Instruction Cache (I-Cache) Miss Rate, Scalar Instruction Ratio, Memory Transfer Engine (MTE) Ratios, and the final compiled Binary Size.

Comparing \textbf{\mysys} (T) against the \textbf{Baseline} (B) and \textbf{LLVM-spec} (L), we attribute the improvements to two primary factors: reduced instruction fetch overhead and simplified runtime calculation.



\paragraph{Reduction in Binary Size and I-Cache Misses}

One of the most significant benefits of \mysys is the reduction in binary size, which directly correlates with improved instruction fetch efficiency.

By propagating runtime invariants, \mysys enables the backend compiler to aggressively perform Dead Code Elimination (DCE) and simplify control flow graphs.

As shown in \autoref{tab:performance_metrics}, operators with complex control logic exhibit dramatic reductions in binary size (e.g., 34.0\% for \texttt{Resize} and 22.7\% for \texttt{D-Attn}).

Consequently, the I-Cache Miss rate for both \texttt{Resize} (9.83\% $\rightarrow$ \textbf{0.00\%}) and \texttt{D-Attn} (4.45\% $\rightarrow$ \textbf{0.00\%}) is completely eliminated, contributing to their respective speedups of 1.18$\times$ and 1.20$\times$.

Notably, even for the computation-heavy \texttt{FFN} operator, specialization reduces the binary size by 20\%, dropping the I-Cache Miss rate from 3.89\% to \textbf{0.00\%}.

For the large kernel \texttt{P-Attn}, \mysys reduces the binary size by over 50KB, decreasing the I-Cache miss rate from 16.40\% to 11.39\%.



\paragraph{Optimization of Computational Intensity and Memory Scheduling}

The \textit{Scalar Ratio} metric indicates the percentage of scalar instructions (used for address calculation and loop control) versus vector/matrix instructions. A lower Scalar Ratio implies that the NPU spends more cycles on actual data processing.

\mysys effectively lowers the Scalar Ratio for operators like \texttt{RMS} (97.9\% $\rightarrow$ 88.6\%), confirming that by resolving loop bounds and offsets at compile time, runtime scalar calculations are converted into immediate operands.

Furthermore, we observe significant improvements in memory scheduling, indicated by the reduction in \textit{MTE2 Ratio} (memory read instructions).

For \texttt{M-Gate} and \texttt{GN}, the MTE2 ratios decrease from 25.0\% to 16.4\% and 25.0\% to 18.8\% respectively.

This suggests that constant specialization facilitates better compiler scheduling, enabling more efficient data tiling and coalesced memory accesses, which directly contributes to their speedups of 1.12$\times$ and 1.17$\times$.



\paragraph{Insensitive Operators}

While most operators benefit from specialization, some, such as \texttt{Pool3D} and \texttt{BMM}, show negligible speedups (1.00$\times$ $\sim$ 1.02$\times$).

For \texttt{Pool3D}, the baseline binary size is already extremely small (13.35KB) with a 0.00\% I-Cache miss rate, leaving virtually no room for control flow simplification.

In the case of \texttt{BMM} (BatchMatMul), the workload is heavily compute-bound, dominated by Matrix Multiplication Unit (Cube) operations.

Although \mysys slightly optimizes the binary size, the overhead of scalar instructions and instruction fetching is already minimal compared to the dense matrix computation.

Therefore, optimizations in the instruction stream do not translate into visible latency reductions when the bottleneck lies strictly in the arithmetic units.



\begin{figure}[H]

    \centering

    \hfill

    \begin{minipage}{0.47\textwidth}

        \includegraphics[width=\linewidth]{figures/evaluation/op-speedup/d_attn_kv_length_speedup.pdf}

        % 图11: 强调针对 Decoding Attention 算子，在不同 KV 长度和配置下的表现

        \caption{Speedup ratio of \texttt{D-Attn} under varying cached KV sequence lengths and configurations.} 

        \label{fig:eval:d_attn_kv}

    \end{minipage}

    \hfill

    \begin{minipage}{0.47\textwidth}

        \includegraphics[width=\linewidth]{figures/evaluation/op-speedup/d_attn_execution_speedup.pdf}

        % 图12: 强调加速比与基准延迟（即任务量）的相关性/分布

        \caption{Distribution of speedup ratios of \texttt{D-Attn} relative to baseline kernel latency.} 

        \label{fig:eval:latency_dist}

    \end{minipage}

    \hfill

\end{figure}





\paragraph{Impact of Workload Scale on Performance}



To understand how \mysys performs under different workload intensities, we analyze the speedup of the \texttt{D-Attn} operator across varying KV cache lengths as \autoref{fig:eval:d_attn_kv} and the relationship between speedup and baseline kernel latency for all operators as \autoref{fig:eval:latency_dist}.

The results demonstrate a clear correlation: the speedup ratio gradually decreases and converges towards 1.0 as the workload (sequence length or execution time) increases.

This trend implies that the performance benefits derived from kernel specialization, primarily the elimination of scalar calculations, control flow overhead and the improvement of instruction cache utilization, are relatively fixed in absolute time and are largely independent of the total task volume.

Consequently, this fixed reduction in overhead translates into significant speedup ratios for small-scale, low-latency kernels, but becomes little by the dominant computation and memory access time in heavy workloads.



\paragraph{Analysis on the compilation pipelines of the kernel specialization}



% TODO:

% 绘制几种算子启动kernel特化后的 IR 指令数目随pass执行的变化情况，其中只标注 JIT spec pass



\subsection{Generalize to other architectures: reducing binary size of AoT compiled operator libraries on GPU}







我的论文的实验的最后一小节是 \subsection{Generalize to other architectures: reducing binary size of AoT compiled operator libraries on GPU}。这里我想讨论的是，\mysys 如何应用到其他的架构上。我需要你为我生成这一节的内容，要点如下：

1. 需要首先承认，应用到诸如 GPU 等其他加速器架构上是困难的，这一点主要来自于：

当代 GPU 等硬件的寄存器资源丰富、标量计算能力强（CUDA Core），尤其重要的是，当代GPU上的主流算子库已经演进的相当成熟，对性能影响较大的参数已经通过预先枚举所有可能的模板参数取值被手工常量特化了（这类参数通常是与控制流相关的参数，特化这些参数可以对循环做完全展开，从而避免控制流导致的SIMT架构不擅长处理的branch，同时还能通过更大范围的指令调度使能更好的流水线调度优化）。

2. 但是，我们发现当前的 GPU 算子库普遍采用 AoT 的编译方式。在编译时，为了支持各种可能的模型、硬件，一个核函数通常需要实例化出很多个版本。但其中的绝大多数在特定的模型、特定的硬件上完全不会调用。这实际上造成了极大的代码冗余。

3. 我们对于 FlashInfer 这一GPU上用于 LLM serving 的关键算子库（vLLM在GPU后端上的默认会调用的算子库）初步适配了 \mysys 。我们基于 \mysys 的 context generation module，从 pytorch前端为 FlashInfer 算子构建了调用上下文，基于 LLVM 编译 FlashInfer 算子程序，并对算子的 Host-side 代码做跨程序的信息传播，并标记所有被发射的核函数对象。最终，我们收集了算子库中所有可能被调用的核函数，并将确定不被调用的核函数的定义以及调用点删除（本质是一个枚举了模板参数取值后实例化的核函数）。最终，我们验证发现 \mysys 最多可以将 FlashInfer 这一算子库裁剪到原体积的 