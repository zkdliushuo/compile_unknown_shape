% 深度学习算法

@article{Liu2022veltairth,
  title={VELTAIR: towards high-performance multi-tenant deep learning services via adaptive compilation and scheduling},
  author={Zihan Liu and Jingwen Leng and Zhihui Zhang and Quan Chen and Chao Li and Minyi Guo},
  journal={Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:246015923},
  volume={""},         
  number={""},
  pages={""},
  numpages={""}, 
  note={No formal volume, number, or pages available}
}

@article{Smith2017DontDT,
  title={Don't Decay the Learning Rate, Increase the Batch Size},
  author={Samuel L. Smith and Pieter-Jan Kindermans and Quoc V. Le},
  journal={ArXiv},
  year={2017},
  volume={abs/1711.00489},
  url={https://api.semanticscholar.org/CorpusID:3516266},
  number={""},
  pages={""},
  numpages={""}, 
  note={No formal volume, number, or pages available}
}

@article{Krizhevsky2017ImageNet,
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
    title = {ImageNet classification with deep convolutional neural networks},
    year = {2017},
    issue_date = {June 2017},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {60},
    number = {6},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/3065386},
    doi = {10.1145/3065386},
    journal = {Commun. ACM},
    month = {may},
    pages = {84–90},
    numpages = {7}
}

@software{Thakkar_CUTLASS_2023,
    author = {Thakkar, Vijay and Ramani, Pradeep and Cecka, Cris and Shivam, Aniket and Lu, Honghao and Yan, Ethan and Kosaian, Jack and Hoemmen, Mark and Wu, Haicheng and Kerr, Andrew and Nicely, Matt and Merrill, Duane and Blasig, Dustyn and Qiao, Fengqi and Majcher, Piotr and Springer, Paul and Hohnerbach, Markus and Wang, Jin and Gupta, Manish},
    license = {BSD-3-Clause},
    month = jan,
    title = {{CUTLASS}},
    url = {https://github.com/NVIDIA/cutlass},
    version = {3.0.0},
    year = {2023},
    organization ={Organization  Unknown},
}

@ARTICLE{he2017fasterrcnn,
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}, 
  year={2017},
  volume={39},
  number={6},
  pages={1137-1149},
  keywords={Proposals;Object detection;Convolutional codes;Feature extraction;Search problems;Detectors;Training;Object detection;region proposal;convolutional neural network},
  doi={10.1109/TPAMI.2016.2577031}}


% 基于解耦表示的，Halide系列工作
@article{ragan2012decoupling,
  title={Decoupling algorithms from schedules for easy optimization of image processing pipelines},
  author={Ragan-Kelley, Jonathan and Adams, Andrew and Paris, Sylvain and Levoy, Marc and Amarasinghe, Saman and Durand, Fr{\'e}do},
  journal={ACM Transactions on Graphics (TOG)},
  volume={31},
  number={4},
  pages={1--12},
  year={2012},
  publisher={ACM New York, NY, USA}
}

@phdthesis{ragan2014decoupling,
  title={Decoupling algorithms from the organization of computation for high performance image processing},
  author={Ragan-Kelley, Jonathan Millard},
  year={2014},
  school={Massachusetts Institute of Technology}
}

@article{adams2019learning,
  title={Learning to optimize halide with tree search and random programs},
  author={Adams, Andrew and Ma, Karima and Anderson, Luke and Baghdadi, Riyadh and Li, Tzu-Mao and Gharbi, Micha{\"e}l and Steiner, Benoit and Johnson, Steven and Fatahalian, Kayvon and Durand, Fr{\'e}do and others},
  journal={ACM Transactions on Graphics (TOG)},
  volume={38},
  number={4},
  pages={1--12},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{anderson2020learning,
  title={Learning to Schedule Halide Pipelines for the GPU},
  author={Anderson, Luke and Adams, Andrew and Ma, Karima and Li, Tzu-Mao and Ragan-Kelley, Jonathan},
  journal={arXiv preprint arXiv:2012.07145},
  year={2020}
}

% 基于解耦表示的

@article{vasilache2019next,
  title={The next 700 accelerated layers: From mathematical expressions of network computation graphs to accelerated GPU kernels, automatically},
  author={Vasilache, Nicolas and Zinenko, Oleksandr and Theodoridis, Theodoros and Goyal, Priya and Devito, Zachary and Moses, William S and Verdoolaege, Sven and Adams, Andrew and Cohen, Albert},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={16},
  number={4},
  pages={1--26},
  year={2019},
  publisher={ACM New York, NY, USA}
}

% Jiang, Ziheng and Moreau, Thierry and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind
@article{chen2018learning,
  title={Learning to optimize tensor programs},
  author={Chen, Tianqi and Zheng, Lianmin and Yan, Eddie and others},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={3389--3400},
  year={2018}
}

@article{vasilache2019next,
  title={The next 700 accelerated layers: From mathematical expressions of network computation graphs to accelerated gpu kernels, automatically},
  author={Vasilache, Nicolas and Zinenko, Oleksandr and Theodoridis, Theodoros and Goyal, Priya and Devito, Zachary and Moses, William S and Verdoolaege, Sven and Adams, Andrew and Cohen, Albert},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={16},
  number={4},
  pages={1--26},
  year={2019},
  publisher={ACM New York, NY, USA}
}


@inproceedings{zheng2021tenset,
 author = {Zheng, Lianmin and Liu, Ruochen and Shao, Junru and Chen, Tianqi and Gonzalez, Joseph and Stoica, Ion and Haj-Ali, Ameer},
 booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
 editor = {J. Vanschoren and S. Yeung},
 pages = {},
 title = {TenSet: A Large-scale Program Performance Dataset for Learned Tensor Compilers},
 url = {https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/a684eceee76fc522773286a895bc8436-Paper-round1.pdf},
 volume = {1},
 year = {2021},
publisher={Publisher Unknown},
address={Conference Location Unknown}
}

@inproceedings{Lee2014predication,
  author={Lee, Yunsup and Grover, Vinod and Krashinsky, Ronny and Stephenson, Mark and Keckler, Stephen W. and Asanovic, Krste},
  booktitle={\bibconf[47th]{MICRO}{2014 47th Annual IEEE/ACM International Symposium on Microarchitecture}}, 
  title={Exploring the Design Space of SPMD Divergence Management on Data-Parallel Architectures}, 
  year={2014}
}

@inproceedings{zhai2023tlp,
author = {Zhai, Yi and Zhang, Yu and Liu, Shuo and Chu, Xiaomeng and Peng, Jie and Ji, Jianmin and Zhang, Yanyong},
title = {TLP: A Deep Learning-Based Cost Model for Tensor Program Tuning},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575737},
doi = {10.1145/3575693.3575737},
abstract = {Tensor program tuning is a non-convex objective optimization problem, to which search-based approaches have proven to be effective. At the core of the search-based approaches lies the design of the cost model. Though deep learning-based cost models perform significantly better than other methods, they still fall short and suffer from the following problems. First, their feature extraction heavily relies on expert-level domain knowledge in hardware architectures. Even so, the extracted features are often unsatisfactory and require separate considerations for CPUs and GPUs. Second, a cost model trained on one hardware platform usually performs poorly on another, a problem we call cross-hardware unavailability. In order to address these problems, we propose TLP and MTL-TLP. TLP is a deep learning-based cost model that facilitates tensor program tuning. Instead of extracting features from the tensor program itself, TLP extracts features from the schedule primitives. We treat schedule primitives as tensor languages. TLP is thus a Tensor Language Processing task. In this way, the task of predicting the tensor program latency through the cost model is transformed into a natural language processing (NLP) regression task. MTL-TLP combines Multi-Task Learning and TLP to cope with the cross-hardware unavailability problem. We incorporate these techniques into the Ansor framework and conduct detailed experiments. Results show that TLP can speed up the average search time by 9.1\texttimes{} and 3.0\texttimes{} on CPU and GPU workloads, respectively, compared to the state-of-the-art implementation. MTL-TLP can achieve a speed-up of 4.7\texttimes{} and 2.9\texttimes{} on CPU and GPU workloads, respectively, using only 7\% of the target hardware data. To the best of our knowledge, TLP is the first tensor program cost model to extract features directly from schedule primitives, and MTL-TLP is the first open-sourced work that effectively addresses the cross-platform unavailability problem. The code is available at https://github.com/zhaiyi000/tlp.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {833–845},
numpages = {13},
keywords = {tensor program, deep Learning, cost model, compiler optimization},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@misc{huawei2021mindspore,
  title={Mindspore Auto Kernel Generator},
  author={Huawei},
  howpublished="\url{https://gitee.com/mindspore/akg/tree/master}",
  year={2021}
}

@article{verdoolaege2013polyhedral,
  title={Polyhedral parallel code generation for CUDA},
  author={Verdoolaege, Sven and Carlos Juega, Juan and Cohen, Albert and Ignacio Gomez, Jose and Tenllado, Christian and Catthoor, Francky},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={9},
  number={4},
  pages={1--23},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@article{zheng2023bladedisc,
    author = {Zheng, Zhen and Pan, Zaifeng and Wang, Dalin and Zhu, Kai and Zhao, Wenyi and Guo, Tianyou and Qiu, Xiafei and Sun, Minmin and Bai, Junjie and Zhang, Feng and Du, Xiaoyong and Zhai, Jidong and Lin, Wei},
    title = {BladeDISC: Optimizing Dynamic Shape Machine Learning Workloads via Compiler Approach},
    year = {2023},
    journal = {Proc. ACM Manag. Data},
    month = {nov}
}

@article{fegade2022cora,
  title={The CoRa tensor compiler: Compilation for ragged tensors with minimal padding},
  author={Fegade, Pratik and Chen, Tianqi and Gibbons, Phillip and Mowry, Todd},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={721--747},
  year={2022}
}

@inproceedings{verdoolaege2010isl,
  title={isl: An integer set library for the polyhedral model},
  author={Verdoolaege, Sven},
  booktitle={International Congress on Mathematical Software},
  pages={299--302},
  year={2010},
  organization={Springer}
}

@misc{loechner1999polylib,
 title={PolyLib: A library for manipulating parameterized polyhedra},
 author={Loechner, Vincent},
 year={1999},
 howpublished="\url{https://repo.or.cz/polylib.git/blob_plain/HEAD:/doc/parampoly-doc.ps.gz}"
}

@techreport{wilde1993polylib,
 author={Doran k. Wilde},
 title ={a library for doing polyhedral operations},
 institution={irisa},
 year={1993},
 number={785},
 month={december}
}

@inproceedings{bondhugula2008automatic,
  title={Automatic transformations for communication-minimized parallelization and locality optimization in the polyhedral model},
  author={Bondhugula, Uday and Baskaran, Muthu and Krishnamoorthy, Sriram and Ramanujam, Jagannathan and Rountev, Atanas and Sadayappan, Ponnuswamy},
  booktitle={International Conference on Compiler Construction},
  pages={132--146},
  year={2008},
  organization={Springer}
}

@inproceedings{bondhugula2008practical,
  title={A practical automatic polyhedral parallelizer and locality optimizer},
  author={Bondhugula, Uday and Hartono, Albert and Ramanujam, Jagannathan and Sadayappan, Ponnuswamy},
  booktitle={Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages={101--113},
  year={2008}
}

@article{grosser2015polyhedral,
  title={Polyhedral AST generation is more than scanning polyhedra},
  author={Grosser, Tobias and Verdoolaege, Sven and Cohen, Albert},
  journal={ACM Transactions on Programming Languages and Systems (TOPLAS)},
  volume={37},
  number={4},
  pages={1--50},
  year={2015},
  publisher={ACM New York, NY, USA}
}

@inproceedings{baghdadi2015pencil,
  title={Pencil: A platform-neutral compute intermediate language for accelerator programming},
  author={Baghdadi, Riyadh and Beaugnon, Ulysse and Cohen, Albert and Grosser, Tobias and Kruse, Michael and Reddy, Chandan and Verdoolaege, Sven and Betts, Adam and Donaldson, Alastair F and Ketema, Jeroen and others},
  booktitle={2015 International Conference on Parallel Architecture and Compilation (PACT)},
  pages={138--149},
  year={2015},
  organization={IEEE}
}

@article{verdoolaege2017scheduling,
  title={Scheduling for PPCG},
  author={Verdoolaege, Sven and Janssens, Gerda},
  journal={Report CW},
  volume={706},
  year={2017}
}

% 综述文章
@article{grondman2012survey,
  title={A survey of actor-critic reinforcement learning: Standard and natural policy gradients},
  author={Grondman, Ivo and Busoniu, Lucian and Lopes, Gabriel AD and Babuska, Robert},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume={42},
  number={6},
  pages={1291--1307},
  year={2012},
  publisher={IEEE}
}

@incollection{van1987simulated,
  title={Simulated annealing},
  author={Van Laarhoven, Peter JM and Aarts, Emile HL},
  booktitle={Simulated annealing: Theory and applications},
  pages={7--15},
  year={1987},
  publisher={Springer}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@inproceedings{wang2019characterizing,
  title={Characterizing deep learning training workloads on alibaba-pai},
  author={Wang, Mengdi and Meng, Chen and Long, Guoping and Wu, Chuan and Yang, Jun and Lin, Wei and Jia, Yangqing},
  booktitle={2019 IEEE International Symposium on Workload Characterization (IISWC)},
  pages={189--202},
  year={2019},
  organization={IEEE}
}

% 深度学习模型

@article{gardner1998artificial,
  title={Artificial neural networks (the multilayer perceptron)—a review of applications in the atmospheric sciences},
  author={Gardner, Matt W and Dorling, SR},
  journal={Atmospheric environment},
  volume={32},
  number={14-15},
  pages={2627--2636},
  year={1998},
  publisher={Elsevier}
}

@article{tai2015improved,
  title={Improved semantic representations from tree-structured long short-term memory networks},
  author={Tai, Kai Sheng and Socher, Richard and Manning, Christopher D},
  journal={arXiv preprint arXiv:1503.00075},
  year={2015}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

% Eksombatchai, Pong and Hamilton, William L and Leskovec, Jure
@inproceedings{ying2018graph,
  title={Graph convolutional neural networks for web-scale recommender systems},
  author={Ying, Rex and He, Ruining and Chen, Kaifeng and others},
  booktitle={\bibconf[24th]{SIGKDD}{ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}},
  pages={974--983},
  year={2018}
}

@inproceedings{graham20183d,
  title={3d semantic segmentation with submanifold sparse convolutional networks},
  author={Graham, Benjamin and Engelcke, Martin and Van Der Maaten, Laurens},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={9224--9232},
  year={2018}
}

% 硬件
@inproceedings{zhang2018deepcpu,
  title={Deepcpu: Serving rnn-based deep learning models 10x faster},
  author={Zhang, Minjia and Rajbhandari, Samyam and Wang, Wenhan and He, Yuxiong},
  booktitle={2018 USENIX Annual Technical Conference (USENIX ATC 18)},
  pages={951--965},
  year={2018}
}

@inproceedings{raina2009large,
  title={Large-scale deep unsupervised learning using graphics processors},
  author={Raina, Rajat and Madhavan, Anand and Ng, Andrew Y},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={873--880},
  year={2009}
}

@article{chen2020survey,
  title={A survey of accelerator architectures for deep neural networks},
  author={Chen, Yiran and Xie, Yuan and Song, Linghao and Chen, Fan and Tang, Tianqi},
  journal={Engineering},
  volume={6},
  number={3},
  pages={264--274},
  year={2020},
  publisher={Elsevier}
}


% 高性能计算库

@misc{intel2020onednn,
  title={oneAPI Deep Neural Network Library},
  author={Intel},
  url="\url{https://github.com/oneapi-src/oneDNN}",
  year={2020}
}

@article{chetlur2014cudnn,
  title={cudnn: Efficient primitives for deep learning},
  author={Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
  journal={arXiv preprint arXiv:1410.0759},
  year={2014}
}

@manual{cambricon2019introduction,
  title={Introduction to CNML and CNRT APIs},
  author={Cambricon},
  howpublished="\url{http://forum.cambricon.com/list-78-1.html}",
  year={2019}
}

@misc{intel2021onednn,
  title={{oneAPI} Deep Neural Network Library},
  author={Intel},
  url={https://github.com/oneapi-src/oneDNN},
  year={2021}
}

@misc{nvidia2019opt,
  title={{CUDA} fundamental optimization},
  author={{NVIDIA}},
  url={https://www.olcf.ornl.gov/wp-content/uploads/2019/12/03-CUDA-Fundamental-Optimization-Part-1.pdf},
  year={2019},
  month=Dec,
}

@misc{nvidia2020roofline,
  title={Roofline Performance Model for HPC and Deep-Learning Applications},
  author={Charlene, Yang and Samuel, Williams and Yunsong, Wang},
  howpublished={Online at \url{https://resources.nvidia.com/gtcd-2020/GTC2020s21565}},
  url={https://resources.nvidia.com/gtcd-2020/GTC2020s21565},
  year={2020},
  month=Dec,
}



@inproceedings{chen2016xgboost,
author = {Chen, Tianqi and Guestrin, Carlos},
title = {XGBoost: A Scalable Tree Boosting System},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939785},
doi = {10.1145/2939672.2939785},
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {785–794},
numpages = {10},
keywords = {large-scale machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}


% 高层应用
@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={arXiv preprint arXiv:1409.3215},
  year={2014}
}

@article{zhang2019deep,
  title={Deep learning based recommender system: A survey and new perspectives},
  author={Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi},
  journal={ACM Computing Surveys (CSUR)},
  volume={52},
  number={1},
  pages={1--38},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@inproceedings{maqueda2018event,
  title={Event-based vision meets deep learning on steering prediction for self-driving cars},
  author={Maqueda, Ana I and Loquercio, Antonio and Gallego, Guillermo and Garc{\'\i}a, Narciso and Scaramuzza, Davide},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5419--5427},
  year={2018}
}

@article{sreenu2019intelligent,
  title={Intelligent video surveillance: a review through deep learning techniques for crowd analysis},
  author={Sreenu, G and Durai, MA Saleem},
  journal={Journal of Big Data},
  volume={6},
  number={1},
  pages={1--27},
  year={2019},
  publisher={SpringerOpen}
}

% 图层优化方法

@inproceedings{qiao2019loop,
  title={From loop fusion to kernel fusion: A domain-specific approach to locality optimization},
  author={Qiao, Bo and Reiche, Oliver and Hannig, Frank and Teich, J{\"\i}rgen},
  booktitle={2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  pages={242--253},
  year={2019},
  organization={IEEE}
}

@misc{tensorflow2021xla,
  title={XLA: Optimizing Compiler for Machine Learning},
  author={Tensorflow},
  year={2021},
  howpublished="\url{https://www.tensorflow.org/xla}"
}

@manual{nvidia2021tensorrt,
  title={TensorRT developer guide},
  author={Nvidia},
  year={2021},
  howpublished="\url{https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html}"
}

% 动态性例证
@article{devarakonda2017adabatch,
  title={AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks},
  author={Aditya Devarakonda and Maxim Naumov and Michael Garland},
  journal={ArXiv},
  year={2017},
  volume={abs/1712.02029},
  url={https://api.semanticscholar.org/CorpusID:3302972},
  number={""},
  pages={""},
  numpages={""}, 
  note={No formal number, or pages available}

}

@article{neubig2017fly,
  title={On-the-fly operation batching in dynamic computation graphs},
  author={Neubig, Graham and Goldberg, Yoav and Dyer, Chris},
  journal={arXiv preprint arXiv:1705.07860},
  year={2017}
}

@article{looks2017deep,
  title={Deep learning with dynamic computation graphs},
  author={Looks, Moshe and Herreshoff, Marcello and Hutchins, DeLesley and Norvig, Peter},
  journal={arXiv preprint arXiv:1702.02181},
  year={2017}
}

@INPROCEEDINGS{ shi2020pointgnn,  
    author={Shi, Weijing and Rajkumar, Raj},  
    booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   
    title={{Point-GNN}: Graph Neural Network for 3D Object Detection in a Point Cloud},
    year={2020},
    volume={},
    number={},
    pages={1708-1716},  
    doi={10.1109/CVPR42600.2020.00178}}


@inproceedings{shi2020point,
  title={Point-gnn: Graph neural network for 3d object detection in a point cloud},
  author={Shi, Weijing and Rajkumar, Raj},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1711--1719},
  year={2020}
}

@inproceedings{shuai2016dag,
  title={Dag-recurrent neural networks for scene labeling},
  author={Shuai, Bing and Zuo, Zhen and Wang, Bing and Wang, Gang},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3620--3629},
  year={2016}
}

@article{smith2017don,
  title={Don't decay the learning rate, increase the batch size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  journal={arXiv preprint arXiv:1711.00489},
  year={2017},
  volume={""},
  number={""},
  pages={""},
  numpages={""}, 
  note={No formal volume, number, or pages available}

}

@article{balles2016coupling,
  title={Coupling adaptive batch sizes with learning rates},
  author={Balles, Lukas and Romero, Javier and Hennig, Philipp},
  journal={arXiv preprint arXiv:1612.05086},
  year={2016},
  volume={""},         
  number={""},
  pages={""},
  numpages={""}, 
  note={No formal volume, number, or pages available}
}

% Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  number={""},
  pages={""},
  numpages={""}, 
  note={No formal volume, number, or pages available}

}

@inproceedings{Brown2020gpt3,
    author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
    title = {Language models are few-shot learners},
    year = {2020},
    isbn = {9781713829546},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
    location = {<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
    series = {NIPS '20},
  pages={""},
  numpages={""}, 
  note={No formal pages available}

}

@article{hannun2014deep,
  title={Deep speech: Scaling up end-to-end speech recognition},
  author={Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and others},
  journal={arXiv preprint arXiv:1412.5567},
  year={2014}
}

@article{zheng2022dietcode,
  title={DietCode: Automatic optimization for dynamic tensor programs},
  author={Zheng, Bojian and Jiang, Ziheng and Yu, Cody Hao and Shen, Haichen and Fromm, Joshua and Liu, Yizhi and Wang, Yida and Ceze, Luis and Chen, Tianqi and Pekhimenko, Gennady},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={848--863},
  year={2022}
}

@article{mu2023haotuner,
  title={HAOTuner: A Hardware Adaptive Operator Auto-Tuner for Dynamic Shape Tensor Compilers},
  author={Mu, Pengyu and Liu, Yi and Wang, Rui and Liu, Guoxiang and Sun, Zhonghao and Yang, Hailong and Luan, Zhongzhi and Qian, Depei},
  journal={IEEE Transactions on Computers},
  year={2023},
  publisher={IEEE},
  volume={""},         
  number={""},
  pages={""},
  numpages={""}, 
  note={No formal volume, number, or pages available}
}

@inproceedings{yu2024mikpoly,
author = {Yu, Feng and Li, Guangli and Zhao, Jiacheng and Cui, Huimin and Feng, Xiaobing and Xue, Jingling},
title = {Optimizing Dynamic-Shape Neural Networks on Accelerators via On-the-Fly Micro-Kernel Polymerization},
year = {2024},
isbn = {9798400703850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620665.3640390},
doi = {10.1145/3620665.3640390},
abstract = {In recent times, dynamic-shape neural networks have gained widespread usage in intelligent applications to address complex tasks, introducing challenges in optimizing tensor programs due to their dynamic nature. As the operators' shapes are determined at runtime in dynamic scenarios, the compilation process becomes expensive, limiting the practicality of existing static-shape tensor compilers. To address the need for effective and efficient optimization of dynamic-shape neural networks, this paper introduces MikPoly, a novel dynamic-shape tensor compiler based on micro-kernel polymerization. MikPoly employs a two-stage optimization approach, dynamically combining multiple statically generated micro-kernels using a lightweight cost model based on the shape of a tensor operator known at runtime. We evaluate the effectiveness of MikPoly by employing popular dynamic-shape operators and neural networks on two representative accelerators, namely GPU Tensor Cores and Ascend NPUs. Our experimental results demonstrate that MikPoly effectively optimizes dynamic-shape workloads, yielding an average performance improvement of 1.49\texttimes{} over state-of-the-art vendor libraries.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {797–812},
numpages = {16},
keywords = {tensor compilers, deep learning},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}
@article{ hochreiter1997lstm,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    year = {1997},
    month = {12},
    pages = {1735-80},
    title = {Long Short-term Memory},
    volume = {9},
    journal = {Neural computation},
    doi = {10.1162/neco.1997.9.8.1735}
}

@inproceedings{ tai2015treelstm,
    title = {Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks},
    author = {Tai, Kai Sheng  and Socher, Richard and Manning, Christopher D.},
    booktitle = {\bibconf[53rd]{ACL}{Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}},
    month = {jul},
    year = {2015},
    address = {Beijing, China},
    publisher = "Association for Computational Linguistics",
  pages={""},
  numpages={""}, 
  note={No formal pages available}

}
    % url = {https://aclanthology.org/P15-1150},
    doi = {10.3115/v1/P15-1150},
    pages = {1556--1566},
}

% 动态网络编译
@inproceedings{roesch2018relay,
  title={Relay: A new ir for machine learning frameworks},
  author={Roesch, Jared and Lyubomirsky, Steven and Weber, Logan and Pollock, Josh and Kirisame, Marisa and Chen, Tianqi and Tatlock, Zachary},
  booktitle={Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  pages={58--68},
  year={2018}
}

@article{lattner2020mlir,
  title={MLIR: A compiler infrastructure for the end of Moore's law},
  author={Lattner, Chris and Pienaar, Jacques and Amini, Mehdi and Bondhugula, Uday and Riddle, River and Cohen, Albert and Shpeisman, Tatiana and Davis, Andy and Vasilache, Nicolas and Zinenko, Oleksandr},
  journal={arXiv preprint arXiv:2002.11054},
  year={2020}
}

# journal={arXiv preprint arXiv:2006.03031},
% Chen, Wei and Wu, Yong and Li, Mu and Sharma, Vin and Tatlock, Zachary and Wang, Yida
@inproceedings{shen2021nimble,
  title={Nimble: Efficiently compiling dynamic neural networks for model inference},
  author={Shen, Haichen and Roesch, Jared and Chen, Zhi and others},
  booktitle = {\bibconf[4th]{MLSys}{Machine Learning and Systems}},
  pages = {208--222},
  volume = {3},
  year={2021},
  publisher={Publisher Unknown},
  address={Conference Location Unknown}
}

@article{fegade2020cortex,
  title={Cortex: A Compiler for Recursive Deep Learning Models},
  author={Fegade, Pratik and Chen, Tianqi and Gibbons, Phil and Mowry, Todd},
  journal={arXiv preprint arXiv:2011.01383},
  year={2020}
}

% Brevdo, Eugene and Burrows, Mike and Davis, Andy and Dean, Jeff and Ghemawat, Sanjay and Harley, Tim and Hawkins, Peter and Isard, Michael and Kudlur, Manjunath and Monga, Rajat and Murray, Derek and Zheng, Xiaoqiang
@inproceedings{ yu2018dynamicflow,
    author = {Yu, Yuan and Abadi, Mart\'{\i}n and Barham, Paul and others},
    title = {Dynamic Control Flow in Large-Scale Machine Learning},
    year = {2018},
    booktitle={\bibconf[13rd]{EuroSys}{Proceedings of the Thirteenth EuroSys Conference}}
}
    % url = {https://doi.org/10.1145/3190508.3190551},
    doi = {10.1145/3190508.3190551},
    articleno = {18},
    numpages = {15},
    location = {Porto, Portugal},
    series = {EuroSys '18}
}

% Sakai, Masahiro and Shimizu, Shunsuke
@inproceedings{ hattori2020shape,
    author = {Hattori, Momoko and Sawada, Shimpei and Hamaji, Shinichiro and others},
    title = {Semi-Static Type, Shape, and Symbolic Shape Inference for Dynamic Computation Graphs},
    year = {2020},
    booktitle = {\bibconf[4th]{MAPL}{ACM SIGPLAN International Workshop on Machine Learning and Programming Languages}},
}
    %doi = {10.1145/3394450.3397465},
    booktitle = {\bibconf[4th]{MAPL}{ACM SIGPLAN International Workshop on Machine Learning and Programming Languages}},
    pages = {11–19},
    numpages = {9},
    keywords = {dynamic languages, dependent types},
    location = {London, UK},
    series = {MAPL 2020}
}

% 其他
@article{zhang2019mobilesurvey,
  title={Deep learning in mobile and wireless networking: A survey},
  author={Zhang, Chaoyun and Patras, Paul and Haddadi, Hamed},
  journal={IEEE Communications surveys \& tutorials},
  volume={21},
  number={3},
  pages={2224--2287},
  year={2019},
  publisher={IEEE}
}

@inproceedings{dai2017deformable,
  title={Deformable convolutional networks},
  author={Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={764--773},
  year={2017}
}

@inproceedings{sun2018pwc,
  title={{PWC-Net: CNNs} for optical flow using pyramid, warping, and cost volume},
  author={Sun, Deqing and Yang, Xiaodong and others},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8934--8943},
  year={2018}
}

@inproceedings{wang2010kernel,
  title={Kernel fusion: An effective method for better power efficiency on multithreaded GPU},
  author={Wang, Guibin and Lin, YiSong and Yi, Wei},
  booktitle={2010 IEEE/ACM Int'l Conference on Green Computing and Communications \& Int'l Conference on Cyber, Physical and Social Computing},
  pages={344--350},
  year={2010},
  organization={IEEE}
}

@book{banzhaf1998genetic,
  title={Genetic programming: an introduction},
  author={Banzhaf, Wolfgang and Nordin, Peter and Keller, Robert E and Francone, Frank D},
  volume={1},
  year={1998},
  publisher={Morgan Kaufmann Publishers San Francisco}
}

@article{medress1977speech,
  title={Speech understanding systems: Report of a steering committee},
  author={Medress, Mark F. and Cooper, Franklin S and Forgie, Jim W. and Green, CC and Klatt, Dennis H. and O'Malley, Michael H. and Neuburg, Edward P and Newell, Allen and Reddy, DR and Ritea, B and others},
  journal={Artificial Intelligence},
  volume={9},
  number={3},
  pages={307--316},
  year={1977},
  publisher={Elsevier}
}

@inproceedings{du2015shidiannao,
  title={ShiDianNao: Shifting vision processing closer to the sensor},
  author={Du, Zidong and Fasthuber, Robert and Chen, Tianshi and Ienne, Paolo and Li, Ling and Luo, Tao and Feng, Xiaobing and Chen, Yunji and Temam, Olivier},
  booktitle={Proceedings of the 42nd Annual International Symposium on Computer Architecture},
  pages={92--104},
  year={2015}
}

@article{han2016eie,
  title={EIE: Efficient inference engine on compressed deep neural network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={243--254},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}

@article{moons2016energy,
  title={An energy-efficient precision-scalable ConvNet processor in 40-nm CMOS},
  author={Moons, Bert and Verhelst, Marian},
  journal={IEEE Journal of Solid-State Circuits},
  volume={52},
  number={4},
  pages={903--914},
  year={2016},
  publisher={IEEE}
}

@article{parashar2017scnn,
  title={Scnn: An accelerator for compressed-sparse convolutional neural networks},
  author={Parashar, Angshuman and Rhu, Minsoo and Mukkara, Anurag and Puglielli, Antonio and Venkatesan, Rangharajan and Khailany, Brucek and Emer, Joel and Keckler, Stephen W and Dally, William J},
  journal={ACM SIGARCH Computer Architecture News},
  volume={45},
  number={2},
  pages={27--40},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{liao2019davinci,
  title={Davinci: A scalable architecture for neural network computing},
  author={Liao, Heng and Tu, Jiajin and Xia, Jing and Zhou, Xiping},
  booktitle={2019 IEEE Hot Chips 31 Symposium (HCS)},
  pages={1--44},
  year={2019},
  organization={IEEE Computer Society}
}

@article{choquette2021nvidia,
  title={NVIDIA A100 Tensor Core GPU: Performance and Innovation},
  author={Choquette, Jack and Gandhi, Wishwesh and Giroux, Olivier and Stam, Nick and Krashinsky, Ronny},
  journal={IEEE Micro},
  year={2021},
  publisher={IEEE}
}

@article{arafa2019cascade,
  title={Cascade lake: Next generation intel xeon scalable processor},
  author={Arafa, Mohamed and Fahim, Bahaa and Kottapalli, Sailesh and Kumar, Akhilesh and Looi, Lily P and Mandava, Sreenivas and Rudoff, Andy and Steiner, Ian M and Valentine, Bob and Vedaraman, Geetha and others},
  journal={IEEE Micro},
  volume={39},
  number={2},
  pages={29--36},
  year={2019},
  publisher={IEEE}
}

@article{gharbi2017deep,
  title={Deep bilateral learning for real-time image enhancement},
  author={Gharbi, Micha{\"e}l and Chen, Jiawen and others},
  journal={ACM Transactions on Graphics},
  volume={36},
  number={4},
  pages={118},
  year={2017},
  publisher={ACM}
}

@inproceedings{dosovitskiy2015flownet,
  title={Flownet: Learning optical flow with convolutional networks},
  author={Dosovitskiy, Alexey and Fischer, Philipp and Ilg, Eddy and Hausser, Philip and Hazirbas, Caner and Golkov, Vladimir and Van Der Smagt, Patrick and Cremers, Daniel and Brox, Thomas},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2758--2766},
  year={2015}
}

@inproceedings{haris2018deep,
  title={Deep back-projection networks for super-resolution},
  author={Haris, Muhammad and Shakhnarovich, Gregory and Ukita, Norimichi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1664--1673},
  year={2018}
}

@inproceedings{zhu2017flow,
  title={Flow-guided feature aggregation for video object detection},
  author={Zhu, Xizhou and Wang, Yujie and Dai, Jifeng and Yuan, Lu and Wei, Yichen},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={408--417},
  year={2017}
}

@inproceedings{hui2018liteflownet,
  title={Liteflownet: A lightweight convolutional neural network for optical flow estimation},
  author={Hui, Tak-Wai and Tang, Xiaoou and Change Loy, Chen},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8981--8989},
  year={2018}
}

@inproceedings{ilg2017flownet,
  title={Flownet 2.0: Evolution of optical flow estimation with deep networks},
  author={Ilg, Eddy and Mayer, Nikolaus and others},
  booktitle={IEEE conference on computer vision and pattern recognition},
  pages={2462--2470},
  year={2017}
}

@article{williams2009roofline,
  title={Roofline: an insightful visual performance model for multicore architectures},
  author={Williams, Samuel and Waterman, Andrew and Patterson, David},
  journal={Communications of the ACM},
  volume={52},
  number={4},
  pages={65--76},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@article{safavian1991survey,
  title={A survey of decision tree classifier methodology},
  author={Safavian, S Rasoul and Landgrebe, David},
  journal={IEEE transactions on systems, man, and cybernetics},
  volume={21},
  number={3},
  pages={660--674},
  year={1991},
  publisher={IEEE}
}

@article{steinberg2009cart,
  title={CART: classification and regression trees},
  author={Steinberg, Dan},
  journal={The top ten algorithms in data mining},
  volume={9},
  pages={179},
  year={2009},
  publisher={CRC Press New York, NY}
}

@article{guide2013cuda,
  title={Cuda c programming guide},
  author={Guide, Design},
  journal={NVIDIA, July},
  year={2013}
}