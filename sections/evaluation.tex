\section{Evaluation}
\label{sec:eval}

Our evaluation mainly focuses on the following questions:
\begin{itemize}
    \item Can \mysys enhance performance across a diverse set of operators and neural networks on Ascend devices?
    \item Can \mysys effectively infer more invariable parameters for kernels with a reasonable compilation overhead compared to current compilers given same context information?
    \item Can \mysys be useful for accelerators beyond Ascend which have high scalar capablity, such as GPUs?
\end{itemize}
Apart from the above main questions, we evaluate the detailed performance metrics differnce brought by \mysys, the performance improvement under different shape configurations.

\subsection{Experimental Setup}
\label{sec:eval:setup}

\paragraph{Hardware and Software Platforms}
The main evaluation is on a platform equipped with an Ascend 910B1 NPU with 64GB HBM, an Intel Core i7-8700 CPU and 64GB DRAM.
The version of Ascend Compute Architecture for Neural Networks(CANN) is 8.0.RC1.
The version of LLVM is 15.0.5 and the version of PyTorch is 2.6.0.
To ensure accuracy, the reported data represents the average of the last 90 runs out of 100 total executions.

\paragraph{Benchmarks}
\label{sec:eval:bench}
To ensure a comprehensive evaluation, our benchmark suite comprises two main components: individual operator latency and end-to-end model inference.

The operator benchmark is derived from mainstream deep learning workloads, encompassing Large Language Models, Computer Vision models, and recommendation models.
\autoref{tab:dynamic_operators} summarizes the categorization and configurations of the evaluated operators.

\input{tables/operator-specs.tex}

Within this set, we particularly emphasize on complex fused operators widely adopted in industrial serving scenarios, such as \texttt{FlashAttention}, \texttt{FeedForward} (FFN), and \texttt{BatchMatMul}.
These operators are characterized by complex implementations and a substantial number of kernel arguments, making them the primary targets for our optimization.
For the end-to-end model evaluation, we select Qwen2-1.5B and Llama2-7B to represent varying scales of modern LLMs.
We assume batch size of LLMs is fixed and the sequence length of the input prompt and kv cache is variable.

\paragraph{Implementation and Experimental Setup}
All evaluations for both operators and models are conducted using PyTorch 2.6.0.
We encapsulate the target operators and models as \texttt{nn.Module} instances.
Dynamic dimensions are explicitly marked using the \texttt{Dynamic} API, and we utilize PyTorch Dynamo with dynamic compilation enabled.

For the operator benchmark, we compare \textbf{\mysys} against the following methods:
\begin{itemize}
    \item \textbf{Baseline:} The standard offline compilation approach. It compiles the operator library based on standard LLVM passes without access to any runtime operator invocation context.
    \item \textbf{LLVM-spec:} A specialization method relying on standard LLVM capabilities. Leveraging the host-side entry function context provided by \mysys, we identify LLVM IR instructions that access constant function arguments and replace them with their corresponding constant values. We then enable the standard LLVM pass pipeline related to constant optimization.
    \item \textbf{Optimal:} An oracle approach serving as the performance upper bound. For each specific input shape, we first execute the operator to capture the actual runtime values of the scalar arguments passed to the kernel. We then perform constant substitution on the kernel's LLVM IR using these captured values and apply the kernel specialization module of \mysys.
\end{itemize}

\subsection{Performance Result of Operators and Models}
\label{sec:eval:result}

\paragraph{Operator Performance Analysis}
\autoref{fig:eval:op-overall} presents the normalized speedup of the evaluated operators across three methods: \textbf{LLVM-spec}, \textbf{\mysys}, and the \textbf{Optimal} upper bound. All results are normalized to the \textbf{Baseline} (standard offline compilation).

Overall, \textbf{\mysys} achieves a geometric mean speedup of \textbf{1.05$\times$} across all operators, significantly outperforming the \textbf{LLVM-spec} baseline (1.01$\times$). Crucially, the performance of \textbf{\mysys} is nearly identical to the \textbf{Optimal} oracle (within 0.5\% difference on average), demonstrating that our framework effectively captures and leverages almost all available runtime constant information for specialization without the heavy overhead of full compilation for each possible shape.

We observe distinct performance behaviors across different operator categories defined in \autoref{tab:dynamic_operators}:

\begin{itemize}
    \item \textbf{Attention mechanisms (P-Attn, D-Attn):} \mysys achieves the most significant gains in attention-related operators, with speedups reaching \textbf{1.15$\times$} for Prefill FlashAttention (P-Attn) and \textbf{1.18$\times$} for Decoding FlashAttention (D-Attn). 
    These kernels heavily rely on tiling parameters (e.g., block sizes derived from sequence lengths).
    While \textbf{LLVM-spec} fails to optimize these due to the complexity of index calculations, \mysys successfully propagates these runtime scalars into the kernel IR, enabling the backend compiler to generate more efficient pipelined instructions.
    
    \item \textbf{Normalization and Up/Down sampling (GN, RMS, Resize):} Computation-intensive element-wise or reduction ops like GroupNorm (GN) and Resize also benefit notably, showing speedups of approximately \textbf{1.11$\times$} and \textbf{1.10$\times$} respectively. The constant specialization allows for the elimination of redundant boundary checks and simplifies division operations related to channel or spatial dimensions.
    
    % \item \textbf{Performance degradation:} As shown in the breakdown (e.g., for \textit{Pool3D} and \textit{P-MM}), \textbf{LLVM-spec} sometimes yields negligible improvements or even slight regressions (speedup $<$ 1.0). This indicates that merely replacing constant arguments without analyzing their propagation paths (as \mysys does) is insufficient to trigger high-level compiler optimizations on the Ascend NPU.
\end{itemize}

\paragraph{End-to-End Model Performance Analysis}
We further evaluate how the operator-level speedups translate to end-to-end model inference performance. \autoref{fig:eval:model-overall} depicts the normalized speedup for \textbf{Llama2-7B} and \textbf{Qwen2-1.5B} across varying sequence lengths and batch sizes (BS=1, 2, 4) during both Prefill and Decode phases.

\textbf{\mysys} consistently improves inference latency across all configurations, with speedups ranging from \textbf{1.01$\times$ to 1.04$\times$}.
While the end-to-end gains are naturally diluted compared to individual kernel speedups (due to non-computational overheads such as kernel launching, memory copying, and unoptimized operators), the improvements remain robust.

Key observations from the model benchmarks include:
\begin{itemize}
    \item \textbf{Consistency across Phases:} Performance gains are observed in both the compute-bound Prefill phase and the memory-bound Decode phase. This confirms that \mysys effectively optimizes both large-GEMM style workloads (via P-Attn and FFN optimizations) and memory-intensive workloads (via D-Attn and Norm optimizations).
    \item \textbf{Impact of Batch Size:} As the batch size increases from 1 to 4, the speedup remains stable. In the Decode phase for Llama2-7B (red line), we observe a slight increase in speedup at certain sequence lengths. This suggests that as the computational intensity increases with larger batches, the benefits of optimized tiling and loop structures generated by \mysys become more pronounced compared to the runtime scheduling overhead.
    
    % \item \textbf{Model Sensitivity:} Qwen2-1.5B generally exhibits similar speedup trends to Llama2-7B, indicating that \mysys is generalizable across different model architectures (e.g., different configurations of GQA or hidden dimensions) without requiring manual tuning.
\end{itemize}

% In summary, \mysys successfully bridges the gap between static compilation and runtime-adaptive optimization, delivering "Optimal-like" performance with negligible overhead.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/evaluation/op-speedup/speedup_histogram.pdf}
    \caption{Overall normalized performance of \mysys compared to compiler baselines }
    \label{fig:eval:op-overall}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/evaluation/model-speedup/multi_model_speedup.png}
    \caption{Overall normalized performance of \mysys compared to PyTorch baseline based on CANN compiler across various dynamic shape operators. For prefill phase, we measure the time to first token (TTFT); for decode phase, we measure the average time per output token(TPOT).}
    \Description{Overall normalized performance of \mysys compared to PyTorch baseline based on CANN compiler across various dynamic shape operators.}
    \label{fig:eval:model-overall}
\end{figure}

\paragraph{Comparison between \mysys and specialization based on Standard LLVM}
% Comparison of performance, compilation overhead, inferred parameters
\subsection{Analysis on Benefits of Kernel Specialization}
\label{sec:eval:analysis}

To deeply understand the sources of the performance gains demonstrated in \autoref{sec:eval:result}, we analyze low-level hardware performance metrics collected during operator execution.
\autoref{tab:performance_metrics} details the Speedup alongside key architectural metrics: Instruction Cache (I-Cache) Miss Rate, Scalar Instruction Ratio, Memory Transfer Engine (MTE) Ratios, and the final compiled Binary Size.
Comparing \textbf{\mysys} (T) against the \textbf{Baseline} (B) and \textbf{LLVM-spec} (L), we attribute the improvements to two primary factors: reduced instruction fetch overhead and simplified runtime calculation.

\paragraph{Reduction in Binary Size and I-Cache Misses}
One of the most significant benefits of \mysys is the reduction in binary size, which directly correlates with improved instruction fetch efficiency.
By propagating runtime invariants, \mysys enables the backend compiler to aggressively perform Dead Code Elimination (DCE) and simplify control flow graphs.
As shown in \autoref{tab:performance_metrics}, operators with complex control logic exhibit dramatic reductions in binary size (e.g., 34.0\% for \texttt{Resize} and 22.7\% for \texttt{D-Attn}).
Consequently, the I-Cache Miss rate for both \texttt{Resize} (9.83\% $\rightarrow$ \textbf{0.00\%}) and \texttt{D-Attn} (4.45\% $\rightarrow$ \textbf{0.00\%}) is completely eliminated, contributing to their respective speedups of 1.18$\times$ and 1.20$\times$.
Notably, even for the computation-heavy \texttt{FFN} operator, specialization reduces the binary size by 20\%, dropping the I-Cache Miss rate from 3.89\% to \textbf{0.00\%}.
For the large kernel \texttt{P-Attn}, \mysys reduces the binary size by over 50KB, decreasing the I-Cache miss rate from 16.40\% to 11.39\%.

\paragraph{Optimization of Computational Intensity and Memory Scheduling}
The \textit{Scalar Ratio} metric indicates the percentage of scalar instructions (used for address calculation and loop control) versus vector/matrix instructions. A lower Scalar Ratio implies that the NPU spends more cycles on actual data processing.
\mysys effectively lowers the Scalar Ratio for operators like \texttt{RMS} (97.9\% $\rightarrow$ 88.6\%), confirming that by resolving loop bounds and offsets at compile time, runtime scalar calculations are converted into immediate operands.
Furthermore, we observe significant improvements in memory scheduling, indicated by the reduction in \textit{MTE2 Ratio} (memory read instructions).
For \texttt{M-Gate} and \texttt{GN}, the MTE2 ratios decrease from 25.0\% to 16.4\% and 25.0\% to 18.8\% respectively.
This suggests that constant specialization facilitates better compiler scheduling, enabling more efficient data tiling and coalesced memory accesses, which directly contributes to their speedups of 1.12$\times$ and 1.17$\times$.

\paragraph{Insensitive Operators}
While most operators benefit from specialization, some, such as \texttt{Pool3D} and \texttt{BMM}, show negligible speedups (1.00$\times$ $\sim$ 1.02$\times$).
For \texttt{Pool3D}, the baseline binary size is already extremely small (13.35KB) with a 0.00\% I-Cache miss rate, leaving virtually no room for control flow simplification.
In the case of \texttt{BMM} (BatchMatMul), the workload is heavily compute-bound, dominated by Matrix Multiplication Unit (Cube) operations.
Although \mysys slightly optimizes the binary size, the overhead of scalar instructions and instruction fetching is already minimal compared to the dense matrix computation.
Therefore, optimizations in the instruction stream do not translate into visible latency reductions when the bottleneck lies strictly in the arithmetic units.

\input{tables/benefits-all}

% 需要增加一个表格

\paragraph{Speedup ratio under different constant dimensions settings}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/evaluation/combined_jit_p_speedup.png}
    \caption{Speedup of D-Attn and P-Attn under different dynamic shape configurations compared to Baseline.}
    \Description{Speedup of \mysys under different dynamic shape configurations compared to Baseline.}
    \label{fig:eval:shape-configs}
\end{figure}

\paragraph{Analysis on the compilation pipelines of the kernel specialization}



\subsection{Generalize to other architectures: reducing binary size of AoT compiled operator libraries on GPU}

