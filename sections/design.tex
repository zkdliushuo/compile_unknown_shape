\section{System Design of \mysys}
\label{sec:design}

% \begin{wrapfigure}{r}{0.5\textwidth} % {r} means right, {0.5\textwidth} sets width to half
%   \vspace{-15pt} % Pull figure up to align better with text top
%   \centering\small
%   \renewcommand{\arraystretch}{1.1} % Slightly adjust for compactness
%   \setlength{\tabcolsep}{3pt} % Reduce column padding to fit narrow width
%   \begin{tabular}{ll}
%     \hline
%     \textbf{Domain} & \textbf{Notation} \\
%     \hline
%     Top-level Variable & $v \in \mathcal{V}$ \\
%     Address-taken Var & $o \in \mathcal{O}$ \\
%     Constant & $c := \text{Int} \mid \text{Float} \mid \text{FuncPtr}$ \\
%     Pointer & $p := (\text{Base, Offsets})$ \\
%     Spec Value & $sv := c \mid p \mid \top \mid \bot$ \\
%     Environment & $\mathbb{E} := \mathcal{V} \mapsto \mathcal{SV}$ \\
%     Store & $\mathbb{S} := \mathcal{O} \mapsto \mathcal{SV}$ \\
%     Calling Context & $\mathbb{C} := (\mathbb{E}, \mathbb{S})$ \\
%     \hline
%   \end{tabular}
%   \caption{Abstract domains and definitions in \mysys.}
%   \label{fig:abstract-domain}
%   \vspace{-10pt} % Reduce whitespace below caption
% \end{wrapfigure}
We propose \mysys, a highly accurate constant propagation framework that infers constant parameter values of kernels from deep learning frameworks, specifically PyTorch.
\mysys takes the ATen IR of the computation graph and the LLVM program of the operator as input.
We first generate the calling context for each operator invocation.
Starting from the entry function of the operator, \mysys performs context-sensitive interprocedural constant propagation, propagating constants through each basic block.
At the end of the analysis, the derived constant values for the concrete parameters of the kernel function are used to perform kernel specialization and optimization.

\begin{table}[htbp]
  \centering
  \small
  \renewcommand{\arraystretch}{1.1}
  \setlength{\tabcolsep}{6pt} % 标准表格稍微宽一点也没关系
  \caption{Abstract domains and definitions in \mysys.}
  \label{fig:abstract-domain}
  \begin{tabular}{ll}
    \hline
    \textbf{Domain} & \textbf{Notation} \\
    \hline
    Top-level Variable & $v \in \mathcal{V}$ \\
    Address-taken Var & $o \in \mathcal{O}$ \\
    Constant & $c := \text{Int} \mid \text{Float} \mid \text{FuncPtr}$ \\
    Pointer & $p := (\text{Base, Offsets})$ \\
    Spec Value & $sv := c \mid p \mid \top \mid \bot$ \\
    Environment & $\mathbb{E} := \mathcal{V} \mapsto \mathcal{SV}$ \\
    Store & $\mathbb{S} := \mathcal{O} \mapsto \mathcal{SV}$ \\
    Calling Context & $\mathbb{C} := (\mathbb{E}, \mathbb{S})$ \\
    \hline
  \end{tabular}
\end{table}

\subsection{Definitions}\label{sec:design:definitions}

To formally describe our analysis, we define the abstract domains and symbols used in \mysys, as summarized in Figure~\ref{fig:abstract-domain}.

\paragraph{Variables and Values.}

We classify variables into two categories: \kw{top-level variables} ($v \in \mathcal{V}$) and \kw{address-taken variables} ($o \in \mathcal{O}$).
A \kw{top-level variable} $v$ corresponds to an SSA (Static Single Assignment) value in LLVM IR, representing a virtual register defined by an instruction or a function argument.
An \kw{address-taken variable} $o$ represents an abstract memory object, such as a stack allocation (alloca) or a global variable, whose address can be manipulated.
The analysis state of these variables is represented by a \kw{specialization value} ($sv \in \mathcal{SV}$).
An $sv$ represents the analysis result of a particular argument or instruction in a specific specialization context, or the value stored at a memory allocation at a program point.
The values in $\mathcal{SV}$ are classified into four categories: constant $c$, pointer $p$, Unknown ($\bot$), or Nothing ($\top$).
A constant $c$ denotes a concrete value, which can be an integer, a floating-point number, or a function pointer.
A pointer $p$ is represented as $\text{Pointer(Base, Offsets)}$, where $\text{Base}$ is an allocation instruction or a global variable, and $\text{Offset}$ is the offset from $\text{Base}$ or $\bot$.

\paragraph{Context and State.}
We define the state of the program using an \kw{Environment} $\mathbb{E}$ and a \kw{Store} $\mathbb{S}$.
The Environment $\mathbb{E}$ maps top-level variables to their specialization values ($\mathcal{V} \mapsto \mathcal{SV}$), tracking the values of virtual registers.
The Store $\mathbb{S}$ maps address-taken variables to specialization values ($\mathcal{O} \mapsto \mathcal{SV}$), modeling the contents of memory.
Consequently, the \kw{Calling Context} $\mathbb{C}$ is defined as a tuple $(\mathbb{E}_{entry}, \mathbb{S}_{entry})$, representing the initial state of the entry function.
Specifically, $\mathbb{C}$ maps the concrete parameters of the kernel function to specialized values: top-level parameters are mapped within $\mathbb{E}_{entry}$, while parameters passed by reference (address-taken) are initialized in $\mathbb{S}_{entry}$.

\subsection{Calling Context Generation for C++ Operators}\label{sec:design:context_construction}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figures/sys-design/context-generation-through-pybind.png}
  \caption{Overview of context generation through PyBind in \mysys. This figure illustrates the workflow for reconstructing the bit-level initial state of C++ operators. Elements with the same background shading denote the same underlying function or class across different stages. The suffix "\_1" of \texttt{add\_1} denotes the first \texttt{add} operator in the model and distinguishes different occurrences of the same operator kind.}
  \Description{A diagram showing the workflow for reconstructing the bit-level initial state of C++ operators, including static analysis, trace analysis, and context synthesis. Elements with the same background shading denote the same underlying function or class across different stages.}
  \label{fig:context_construction}
\end{figure}

\autoref{fig:context_construction} illustrates the overall workflow for reconstructing the bit-level initial state ($\mathbb{C}_{init}$) of C++ operators. 
The workflow bridges the gap between high-level Python execution and low-level C++ memory states through a hybrid approach.
As shown in \autoref{fig:context_construction}, the context generation module of \mysys consists of three core components highlighted with a \textbf{green background}, namely \textbf{Static Analysis}, \textbf{Trace Analysis}, and \textbf{Context Synthesis}. 
In the figure, elements with identical background shading refer to the same underlying function or class, consistently tracked across the different stages of the pipeline.
The process is divided into an offline preparation phase and an online synthesis phase.

\paragraph{Offline phase: static analysis of binding APIs and memory layout}
The offline phase (top of \autoref{fig:context_construction}) builds a comprehensive knowledge base of the codes of C++ operator library.
We employ a \textbf{static analysis} component (implemented based on Clang) to scan the library source code.
This module produces two key outputs:
\begin{itemize}
  \item \textbf{C++ Operator Metadata}: It locates Python–C/C++ bindings (e.g., mapping Python’s \texttt{add} to its C++ implementation) and records the corresponding function signatures.
  \item \textbf{Memory Layout of C++ Types}: It recursively parses the AST to recover concrete memory layouts. 
  For example, for the \texttt{Tensor} type, it infers a pointer to \texttt{TensorImpl}, which in turn contains nested structures such as \texttt{sizes} (within \texttt{std::vector<int>}).
\end{itemize}

\paragraph{Online phase: trace analysis and context synthesis}
The online phase (bottom of \autoref{fig:context_construction}) captures runtime information from the DL model to instantiate the context.

\textbf{trace analysis.}
During model execution (e.g., \texttt{torch.add(a, b, ...)}), we invoke the \textbf{trace analysis} component (built on top of the tracing mechanism of compilers such as Inductor) to inspect the execution graph.
Unlike standard execution, this stage performs symbolic execution to preserve information about symbolic shapes.
For example, as illustrated in \autoref{fig:context_construction}, the input script marks tensor \texttt{b} as dynamic via \texttt{mark\_dynamic}.
Consequently, the framework’s runtime knows that the first dimension of \texttt{a} is a concrete constant \texttt{4} (from the Python layer), whereas the corresponding dimension of \texttt{b} is symbolic (unknown at compile time, denoted as ?) as indicated by the framework’s symbolic engine.

\textbf{context synthesis.}
Finally, the \textbf{context synthesis} module combines the offline memory layouts with the online argument values. 
Using a set of predefined \textbf{tensor instantiation rules} (which describe how to allocate and initialize objects), it generates the final \textbf{calling context for c++ operator}, i.e., context for \texttt{add\_1} in \autoref{fig:context_construction}.
The synthesizer reconstructs pointer chains ($o_1 \rightarrow o_2 \rightarrow \dots$) and populates memory slots.
It explicitly preserves uncertainty observed in the trace: static values (such as \texttt{4}) are written to their exact offsets, while dynamic dimensions are written as unknowns (denoted as \texttt{?} in the memory graph), ensuring the soundness of subsequent static analysis.

As a concrete example, we show the instantiation rule for a \texttt{Tensor} argument.
Given a tensor and its shape list $L = [d_0, \dots, d_{n-1}]$ from the trace (where each $d_k$ may be a constant or a symbolic dimension), the following rule allocates the corresponding C++ objects and populates their fields in the initial context $\mathbb{C}_{init}$, which consists of the abstract environment $\mathbb{E}$ and store $\mathbb{S}$:

\begin{equation}
  \textsc{[Tensor]} \quad
  \frac{
    \begin{aligned}
      o_{t}&, o_{impl}, o_{data}~ \text{ are fresh} \\
      L &= [d_0, \dots, d_{n-1}] \\
      \mathbb{E}'& = \mathbb{E}[v \mapsto \addr{o_{t}}{\mathbf{0}}]
    \end{aligned}
    \qquad
    % Part 3: Store Update (Right)
    \mathbb{S}' = \mathbb{S} \left[ 
      \begin{aligned}
        \addr{o_{t}}{\mathbf{0}} & \mapsto \addr{o_{impl}}{\mathbf{0}}, \\
        \addr{o_{impl}}{\delta_{sz}} & \mapsto \addr{o_{data}}{\mathbf{0}}, \\
        \addr{o_{impl}}{\delta_{sz} + \delta_{cap}} & \mapsto n, \\
        \forall k \in [0, n).\; \addr{o_{data}}{k \cdot w} & \mapsto \alpha(d_k)
      \end{aligned}
    \right]
  }{
    % Conclusion
    \langle \textsc{Instantiate}(v, L), \compstate{\mathbb{E}}{\mathbb{S}} \rangle \longrightarrow \compstate{\mathbb{E}'}{\mathbb{S}'}
  }
\end{equation}
Here, $o_t$ models the top-level \texttt{Tensor} object, $o_{impl}$ models its internal \texttt{TensorImpl}, and $o_{data}$ models the contiguous array storing tensor sizes (e.g., the buffer of \texttt{std::vector<int64\_t>}).
$\delta_{sz}$ is the offset of the \texttt{sizes} field inside \texttt{TensorImpl}, and $w$ is the stride (in bytes) of an \texttt{int} element.
The helper $\alpha$ writes concrete shape dimensions as constants and maps symbolic dimensions to $\top$, conservatively representing “unknown but live” values in the abstract domain.
This rule-based instantiation mechanism generalizes to other framework-specific types and forms the backbone of our C++ calling-context synthesis.

\textbf{Why predefined rules for C++ calling-context synthesis?}
Although trace analysis preserves rich symbolic information for Python-level arguments (e.g., symbolic tensor dimensions), this information is disconnected from the concrete memory layout used by C++ operator implementations.
In particular, there is no automatically available mapping from a framework-level symbolic variable to its exact byte offset and container object in the C++ layout (e.g., an element of \texttt{TensorImpl::sizes\_} inside a nested \texttt{std::vector<int64\_t>}).
Reconstructing such a mapping would require precisely tracking how values flow from the Python runtime into C++ auxiliary buffers.
These behaviors are dispersed across the Python–C/C++ boundary and lose high-level semantics, making a fully automatic and sound “Python symbol $\leftrightarrow$ C++ field offset” mapping impractical.

Therefore, \mysys adopts a pragmatic design: for a small set of core C++ types (such as \texttt{Tensor}), we manually define instantiation rules that specify how Python-level argument values (including symbolic dimensions) are embedded into the C++ memory state.
In practice, the parameter types of C++ operators are very limited—primarily \texttt{Tensor}, several container types (e.g., \texttt{std::vector<T>}), and scalar types—so a small number of hand-written rules suffice to support a wide range of operators across libraries.
These core abstractions are also highly stable and rarely change, making the rule set robust and low-maintenance.
Operator developers provide these rules, and the context synthesis module interprets them to construct $\mathbb{C}_{init}$ in a sound and repeatable way.

\subsection{Identify Early Return Pattern}

\input{algorithm/early-return}

The early-return path bypasses the normal execution path, and these two paths often join at the function's ending block. 
At join points, merging values from both paths results in losing specialized constant information. 
Identifying these early-return paths allows avoiding merging value information on these paths, thereby preserving only the constant values produced on the normal execution path.

As a result, we introduce Algorithm~\ref{algo:runtime-shape-check-detection} to identify and label early-return edges in the control-flow graph.
After annotating edges as either Normal or Early-Return, the following constant propagation can selectively merge incoming values only from Normal edges. All successor basic blocks reachable solely via early-return edges are excluded, thereby preventing them from affecting the join-point computations.


Algorithm~\ref{algo:runtime-shape-check-detection} detects early-return edges derived from runtime shape checks in the given function $F$. 
It starts from the exit block of $F$ and assumes the return value is a \texttt{phi} instruction node.
Each incoming pair \((v, P)\) of the \texttt{phi} corresponds to a value \(v\) from a predecessor block \(P\).
If \(v\) is a compile-time constant in the error set \(\mathcal{E}\), the predecessor block \(P\) belongs to an early-return path.
Otherwise, it belongs to a normal path.
Then, a backward propagation (\texttt{PropagateBack}) process marks corresponding labels for all edges on these paths.
Note that a normal mark will overwrite any previous early-return marks for soundness.


However, simply identifying early-return edges in a single function is not sufficient.
In some cases, the early returns may be incurred because of the invocation of a subroutine. 
Therefore, the algorithm has to recursively identify which subroutine generates the early return path.
Obviously, basic blocks that contain such sub-routines have both normal and early-return outgoing edges.
The algorithm extracts the condition value from the last conditional branch instruction in such blocks. 
Assuming the condition value is of the form \(v \bowtie s\), where \(\bowtie\ \in \{eq, neq\}\), the algorithm checks if it indicates 
\(s\) belongs to \(\mathcal{E}\) when \(\bowtie\) is \(eq\), or if \(s\) belongs to the success state set \(\mathcal{S}\) when \(\bowtie\) is \(neq\). 
If these conditions hold and \(v\) is a return value from a function call instruction, the algorithm recursively applies its analysis to that function. 
This method employs a reverse control-flow traversal to capture the complete set of reachable edges and integrates status code evaluation to accurately identify segments affected by runtime shape checks.

Algorithm~\ref{algo:runtime-shape-check-detection} relies on several helper functions.
\texttt{GetPhiOperand} extracts the operand list of a \texttt{phi} instruction in the exit block, pairing each incoming value with its predecessor block.  
\texttt{IsConstant} checks whether a value is a compile-time constant.  
\texttt{MarkNormalEdge} and \texttt{MarkEarlyReturnEdge} are used to annotate edges with their respective roles in the control-flow graph. 
\texttt{IsNormal} and \texttt{IsEarlyReturn} query the current marking of an edge.  
\texttt{ReverseReachableEdges} returns all edges that can reach a given block in reverse, allowing backward propagation of the edge property.  

\input{algorithm/intra-procedural-analysis}
\subsection{Inter-procedural constant propagation}


In this section, we present our improved constant propagation algorithm, which integrates precise control flow and context-sensitive analysis to reduce active edges and merge points. 
As shown in Algorithm~\ref{algo:const-propagation}, it operates on the control flow graph (CFG) of a function $F$, starting from the entry block and processing blocks via a worklist $\mathcal{W}$.

For each basic block $B$, the algorithm first computes an input context $\mathbb{S}_{in}$ by merging contexts from predecessor blocks through the function \texttt{MergePredecessors}. 
The block is then processed using \texttt{RunOnBasicBlock} to yield an output context $\mathbb{S}_{out}$ and a flag $updated$ indicating whether the context has been updated.
If the context has changed or the algorithm is not in maximal fixed-point (MFP) mode, the successors of $B$ are pushed into the worklist using \texttt{ProcessSuccessors}.

The key function \texttt{RunOnBasicBlock} implements three distinct processing components:

\begin{itemize}
    \item Arithmetic Instruction Handling:
    Performs straightforward semantic-based constant propagation for fundamental operations through direct value substitution.
    \item Pointer-Centric Instruction Handling: track the pointers' access paths, as detailed in the next section. 
    \item Function Call: Update the environment $\mathbb{E}$ of the function entry basic block according to the actual parameters of the call point, and then call the function \texttt{RunOnFunction} for processing
\end{itemize}


A key component of our approach is the \texttt{SuccessorLiveness} function, which evaluates the reachability of each edge from $B$ to a successor $S$ based on the specialized context $\mathbb{S}_{out}$ to determine the liveness of successor block $S$ from current block $B$.
This function returns a value $C \in \{\texttt{true}, \texttt{false}, \texttt{unknown}\}$, reflecting whether the branch is definitely reachable, definitely unreachable, or uncertain. 
In non-MFP mode, only successors with $C=\texttt{true}$ are processed.
If $C$ is \texttt{unknown}, the algorithm switches to MFP mode and sets the final merge point $\mathcal{M}$ to the direct post-dominating block of $B$.
In MFP mode, every successor $S\neq\mathcal{M}$ with $C\neq\texttt{false}$ is added to the worklist, and the corresponding edge is marked as active.

This design offers finer control over path merging by filtering out provably unreachable edges. 
The precision of constant propagation is improved by avoiding the propagation of imprecise contexts from definitely unreachable paths.
Moreover, the adaptive switch to MFP mode ensures that the analysis remains conservative when the branching target is uncertain.
Enhanced memory alias analysis further increases the accuracy of condition evaluation, leading to better overall performance in our constant propagation.
