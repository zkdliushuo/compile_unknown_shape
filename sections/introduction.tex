\section{Introduction}\label{sec:intro}

With the rapid advancement of deep learning, large-scale models such as Llama 3, DeepSeek, Qwen, and OpenSora have demonstrated exceptional performance across diverse tasks. To accelerate model training and inference while providing substantial computational power, major companies have developed domain-specific architectures (DSAs), including NVIDIA GPUs, Huawei Ascend NPUs, Google TPUs, and Cambricon MLUs. Among these, Ascend NPUs have achieved widespread adoption in supporting influential models like Qwen and DeepSeek series, delivering high performance with low power consumption.

Ascend's architectural innovation lies in its programmable hardware components that enable efficient computation across diverse workloads. These include: 1) computation units featuring Scalar units for control flow processing and instruction dispatching, Vector units for vectorized operations, and Cube units for matrix computations; and 2) flexible memory hierarchy with specialized buffers (L0 A/B/C for Cube units and Unified buffer for Vector units) and efficient data movement mechanisms between storage tiers (detailed in Section~\ref{sec:background}).

While this design offers superior programmability and optimization potential compared to general-purpose GPUs, it introduces significant challenges in kernel optimization. Developers must carefully manage parallelism and load balancing through parameters controlling tile sizes, iteration orders, buffer counts, and data movement patterns across memory hierarchies. For dynamic-shape models (e.g., Transformers with runtime-variable tensor dimensions), these control parameters, along with shape parameters, ragged tensor offsets, and data formats, become formal arguments of kernel functions. Runtime determination of these parameters consumes valuable scalar registers (only 32 general-purpose 64-bit registers per Davinci core) and creates performance bottlenecks in scalar computation units, particularly evident in workloads like Flash Decoding where complex control flows dominate over compute operations.

Our analysis of CANNDev and CANN OPS ADV operator libraries reveals critical insights: 1) Hardware utilization metrics show scalar unit bottlenecks limit parallel execution of compute and memory operations; 2) Most kernel parameters become compile-time constants when model configurations (e.g., attention heads, embedding sizes) and hardware specifications are fixed. This observation motivates our key contribution: a constant specialization framework that identifies compile-time constants in kernel parameters and generates optimized kernels through DSA-native compilation.

Existing approaches for parameter specialization face significant limitations: manual constant propagation causes code bloat through exhaustive enumeration, while auto-tuning/immediate compilation introduces runtime overhead. We propose \mysys, a cross-procedural constant propagation framework for C++-based operator libraries that achieves two objectives: 1) Reduces scalar computation overhead and inference latency; 2) Minimizes code size for resource-constrained deployment scenarios. Our technical innovations include: (1) Symbolic shape analysis through dynamic model execution in frameworks like PyTorch Dynamo; (2) Abstract interpretation framework with pointer analysis for precise constant propagation; (3) Aggressive optimization through unreachable path elimination and conservative pointer handling; and (4) Runtime validation mechanism ensuring correctness while maintaining performance benefits.

Experimental results demonstrate that our approach effectively addresses scalar unit bottlenecks while maintaining compatibility with dynamic shape models. The framework enables efficient specialization of kernel parameters through compile-time constant identification, achieving performance improvements without compromising deployment flexibility.
