\section{Background}
\label{sec:overview}

\subsection{Ascend Architecture and Challenge}

\begin{figure}
    \includegraphics[width=0.7\textwidth]{figures/motivation/ascend-data-path.png}
    \caption{Overview of Ascend Architecture. The figure illustrates the key components of the Ascend architecture.}
    \label{fig:ascend_architecture}
\end{figure}

The Ascend architecture as shown in \autoref{fig:ascend_architecture} is designed to efficiently handle a variety of deep learning workloads by incorporating specialized hardware components. The key components of the Ascend architecture include:
\begin{itemize}
    \item \textbf{Computation Units:} The architecture features several computation units, including Scalar Units for control flow processing and instruction dispatching, Vector Units for vectorized operations, and Cube Units for matrix computations. These units work together to execute complex deep learning operations efficiently.
    
    \item \textbf{Memory Hierarchy:} Ascend employs a multi-tiered memory hierarchy with specialized buffers. The L0 A/B/C buffers are dedicated to Cube Units, while the Unified Buffer serves Vector Units. This design allows for efficient data movement between different storage tiers, optimizing memory access patterns for various workloads.
    
    \item \textbf{Data Movement Mechanisms:} The architecture includes flexible data movement mechanisms that facilitate efficient transfer of data between computation units and memory buffers. This is crucial for maintaining high throughput and minimizing latency during deep learning operations.
\end{itemize}

These components work together to provide a programmable and high-performance platform for deep learning applications, enabling developers to optimize their workloads effectively.

While such programmability and optimization potential surpass that of general-purpose GPUs, it introduces significant challenges in kernel optimization. 
Developers must carefully manage parallelism and load balancing through parameters controlling tile sizes, iteration orders, buffer counts, and data movement patterns across memory hierarchies. 
For dynamic-shape models (e.g., Transformers with runtime-variable tensor dimensions), these control parameters, along with shape parameters, ragged tensor offsets, and data formats, become formal arguments of kernel functions. 
At the beginning of kernel execution, these parameters need to be loaded from the global memory of the device to registers and the unified buffer, which then executes control flow instructions and scalar computations dependent on these parameter variables.
This process involves many load and store operations, consuming valuable scalar registers, thus leading to considerable initialization overhead on the Scalar Unit.
In workloads with low computational intensity, the proportion of such initialization overhead becomes significant such as Flash Decoding, whose number of parameters is large and single execution time is small.

We have analyzed that since most kernel parameters are actually compile-time constants when model configurations (e.g., attention heads, embedding sizes) and hardware specifications are fixed, there exists a potential opportunity to accelerate Ascend kernels by identifying these compile-time constants and generating optimized kernels through DSA-native compilation.
However, we find it difficult to automatically identify compile-time constant parameters for operator programs based on C++ implementation, which we will discuss in the next subsection.

\subsection{Current Kernel Specialization Methods}

Currently, there are two common methods for specializing kernel parameters in existing DSA operator libraries. 
The first is Ahead-of-Time(AoT) compilation, enumerating certain critical kernel parameters (e.g., tile size) before the execution stage and precompiling all the specialized kernels.
The second method determines the parameter values at runtime and performs just-in-time (JIT) compilation based on these values.
\autoref{lst:compare_aot_jit_without_graph} shows an example of specializing the \kw{T} parameter in a simple addition kernel using both methods, where the kernel function \kw{add\_kernel} is a template function parameterized by the tile size \kw{T}.

\begin{figure}
\begin{minipage}{0.38\textwidth}
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize]
template <int T>
void add_kernel(float* A, float* B, float* C, int N) {
  for (int i = 0; i < N; i += T) {
    for (int j = 0; j < T; ++j) {
      int idx = i + j;
      if (idx < N)
        C[idx] = A[idx] + B[idx];
    }
  }
}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{0.57\textwidth}
\begin{lstlisting}[firstnumber=11, basicstyle=\ttfamily\footnotesize]
void aot_add(float* A, float* B, float* C, int N) {
  switch (dispatch_tile_size(N)) {
    case 16: add_kernel<16>(A, B, C, N); break;
    case 32: add_kernel<32>(A, B, C, N); break;
    case 64: add_kernel<64>(A, B, C, N); break;
}}
void jit_add(float* A, float* B, float* C, int N) {
  int T = autotune_tile_size(N);
  auto kernel = run_compile(add_kernel, T);
  kernel(A, B, C, N);
}
\end{lstlisting}
\end{minipage}
\caption{Comparison of AoT compilation and JIT compilation for kernel specialization without graph execution.}
\label{lst:compare_aot_jit_without_graph}
\end{figure}

The AoT compilation method, a.k.a. the function \kw{aot\_add}, precompiles specialized kernels for all the possible return values of the function \kw{dispatch\_tile\_size}\footnote{The function \kw{dispatch\_tile\_size} is generally a hand-written dispatch function that maps an input shape to its optimal tiling configurations.}.
However, it can lead to a significant increase in binary size due to the large number of specialized kernels.
Given a model and hardware setup, the shape parameter N is often fixed, as a result only one specialized kernel is actually needed during execution, while all other specialized kernels are redundant.

On the other hand, the JIT compilation method, a.k.a. the function \kw{jit\_add}, provides flexibility by determining parameter values at runtime. 
This allows the kernel to adapt to varying input shapes and hardware configurations.
However, the runtime compilation process introduces additional overhead, which can impact performance, especially for workloads with low computational intensity or frequent kernel launches.

\begin{figure}
\centering
\begin{minipage}{0.7\textwidth}
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize]
void add_kernel(float* A, float* B, float* C, int* N, int* T) {
  for (int i = 0; i < *N; i += *T)
    for (int j = 0; j < *T; ++j)
      if (i + j < *N) C[i + j] = A[i + j] + B[i + j];
}
void graph_add(float* A, float* B, float* C, int N) {
    int T = dispatch_tile_size(N);
    add_kernel(A, B, C, &N, &T);
}
\end{lstlisting}
\end{minipage}
\caption{Implementation of the add operator under the graph execution mode.}
\label{lst:add_with_graph}
\end{figure}

Given these limitations, we believe it is necessary to design a constant specialization framework that can automatically identify compile-time constant kernel parameters based on the input shapes and host code of operators, enabling automatic constant specialization of kernels without incurring significant code bloat or runtime overhead, especially for Ascend operator libraries where kernel parameters are too many to be enumerated.

\subsection{Existing Constant Propagation Techniques}

Since neither manual constant propagation nor runtime JIT compilation adequately addresses the challenges of constant specialization in operator libraries, 
it is necessary to design a specialization framework based on static program analysis at the offline compilation stage. Existing constant propagation techniques can be broadly categorized into two primary methodologies:

\begin{itemize}
    \item \textbf{Data Flow Analysis (DFA):} A classic algorithm for constant propagation, DFA models the program as a Control Flow Graph (CFG) and propagates values until a fixed-point is reached. Its theoretical foundation, Abstract Interpretation, ensures soundness and termination by defining abstract value domains (e.g., \textit{Top}, \textit{Constant}, \textit{NAC}) and \texttt{join} operators. While DFA is computationally efficient, it is \textbf{path-insensitive}, leading to a loss of precision at control-flow merge points.

    \item \textbf{Symbolic Execution (SE):} A \textbf{path-sensitive} methodology that explores individual program paths, maintaining variable states as symbolic expressions and tracking branch constraints. SE offers high precision and can identify path-dependent constants missed by DFA. However, it suffers from the \textbf{path explosion} problem, making it computationally expensive and often impractical for this task.
\end{itemize}

However, existing constant propagation techniques are primarily designed with general-purpose programs in mind, and they often fail to meet the stringent requirements of operator libraries, where the number of deduced constant values is critical. The main challenges that hinder their effectiveness in this context include the following aspects:
\begin{itemize}
    \item \textbf{Dynamic Shape Checks:} Operator programs frequently involve dynamic shape checks, resulting in multiple program paths. Accurately identifying and eliminating unreachable paths is essential to maintain precision during constant propagation.
    \item \textbf{Precise Pointer Analysis:} C++ operator programs heavily rely on polymorphic types and pointers. It is crucial to perform precise pointer analysis throughout the constant propagation process, including resolving function pointers and determining which fields of struct objects are being accessed.
    \item \textbf{Handling Uncertain Pointer Targets:} When faced with uncertain pointer targets, traditional methods often mark all potential memory regions as \kw{Unknown}, which can contaminate the analysis and reduce its effectiveness.
\end{itemize}

The above challenges necessitate a specialized constant propagation framework tailored for C++-based operator libraries, which motivates us to develop \mysys as shown in \autoref{sec:design}.
