\section{Background}
\label{sec:overview}

\subsection{Ascend Architecture and Challenge}

\begin{figure}
    \includegraphics[width=\textwidth]{figures/motivation/ascend-data-path.png}
    \caption{Overview of Ascend Architecture. The figure illustrates the key components of the Ascend architecture.}
    \label{fig:ascend_architecture}
\end{figure}

The Ascend architecture as shown in \autoref{fig:ascend_architecture} is designed to efficiently handle a variety of deep learning workloads by incorporating specialized hardware components. The key components of the Ascend architecture include:
\begin{itemize}
    \item \textbf{Computation Units:} The architecture features several computation units, including Scalar Units for control flow processing and instruction dispatching, Vector Units for vectorized operations, and Cube Units for matrix computations. These units work together to execute complex deep learning operations efficiently.
    
    \item \textbf{Memory Hierarchy:} Ascend employs a multi-tiered memory hierarchy with specialized buffers. The L0 A/B/C buffers are dedicated to Cube Units, while the Unified Buffer serves Vector Units. This design allows for efficient data movement between different storage tiers, optimizing memory access patterns for various workloads.
    
    \item \textbf{Data Movement Mechanisms:} The architecture includes flexible data movement mechanisms that facilitate efficient transfer of data between computation units and memory buffers. This is crucial for maintaining high throughput and minimizing latency during deep learning operations.
\end{itemize}

These components work together to provide a programmable and high-performance platform for deep learning applications, enabling developers to optimize their workloads effectively.

While such programmability and optimization potential surpass that of general-purpose GPUs, it introduces significant challenges in kernel optimization. 
Developers must carefully manage parallelism and load balancing through parameters controlling tile sizes, iteration orders, buffer counts, and data movement patterns across memory hierarchies. 
For dynamic-shape models (e.g., Transformers with runtime-variable tensor dimensions), these control parameters, along with shape parameters, ragged tensor offsets, and data formats, become formal arguments of kernel functions. 
At the beginning of kernel execution, these parameters need to be loaded from the global memory of the device to registers and the unified buffer, which then executes control flow instructions and scalar computations dependent on these parameter variables.
This process involves many load and store operations, consuming valuable scalar registers, thus leading to considerable initialization overhead on the Scalar Unit.
In workloads with low computational intensity, the proportion of such initialization overhead becomes significant such as Flash Decoding, whose number of parameters is large and single execution time is small.

We have analyzed that since most kernel parameters are actually compile-time constants when model configurations (e.g., attention heads, embedding sizes) and hardware specifications are fixed, there exists a potential opportunity to accelerate Ascend kernels by identifying these compile-time constants and generating optimized kernels through DSA-native compilation.
However, we find it difficult to automatically identify compile-time constant parameters for operator programs based on C++ implementation, which we will discuss in the next subsection.

\subsection{Current Kernel Specialization Methods}

Currently, there are two common methods for specializing kernel parameters in DSA operator libraries. The first method involves enumerating certain critical kernel parameters (e.g., tile size) during the offline stage and precompiling the corresponding specialized kernels.
The second method determines the parameter values at runtime and performs just-in-time (JIT) compilation based on these values.

\begin{minipage}{0.47\textwidth}
\begin{lstlisting}[language=C++, basicstyle=\scriptsize, caption={Offline compilation.}, label={lst:offline_compilation}]
void add_kernel(float* A, float* B, float* C, int N) {
    int TILE_SIZE = calculate_tile_size(N);
    switch (TILE_SIZE) {
        case 16: add_kernel<16>(A, B, C, N); break;
        case 32: add_kernel<32>(A, B, C, N); break;
        case 64: add_kernel<64>(A, B, C, N); break;
        default: throw std::runtime_error("Unsupported TILE_SIZE");
    }
}
template <int TILE_SIZE>
void add_kernel(float* A, float* B, float* C, int N) {
    for (int i = 0; i < N; i += TILE_SIZE) {
        for (int j = 0; j < TILE_SIZE && i + j < N; ++j) {
            C[i + j] = A[i + j] + B[i + j];
        }
    }
}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{0.47\textwidth}
\begin{lstlisting}[language=C++, basicstyle=\scriptsize, caption={Runtime JIT compilation.}, label={lst:runtime_jit}]
void add_kernel(float* A, float* B, float* C, int N) {
    int TILE_SIZE = auto_tune_tile_size(N);
    auto compiled_kernel = jit_compile_kernel(TILE_SIZE);
    compiled_kernel(A, B, C, N);
}
std::function<void(float*, float*, float*, int)> 
jit_compile_kernel(int TILE_SIZE) {
    return [TILE_SIZE](float* A, float* B, float* C, int N) {
        for (int i = 0; i < N; i += TILE_SIZE) {
            for (int j = 0; j < TILE_SIZE && i + j < N; ++j) {
                C[i + j] = A[i + j] + B[i + j];
            }
        }
    };
}
\end{lstlisting}
\end{minipage}

\autoref{lst:offline_compilation} shows an example of the offline compilation method, where the kernel function \kw{add\_kernel} specializes the \kw{TILE\_SIZE} parameter by enumerating its possible values and precompiling the corresponding specialized kernels. 
This method is effective when the number of parameters to be specialized is small and their possible values can be easily enumerated. 
However, it can lead to code bloat as the number of specialized kernels increases.
\autoref{lst:runtime_jit} illustrates the runtime JIT compilation method, where the \kw{TILE\_SIZE} parameter is determined at runtime using an auto-tuning function, and the kernel is compiled just-in-time based on this value.
This approach allows for greater flexibility and adaptability to different input shapes and hardware configurations but incurs runtime overhead due to the compilation process.

Given these limitations, we believe it is necessary to design a constant specialization framework that can automatically identify compile-time constant kernel parameters based on the input shapes and host code of operators, enabling automatic constant specialization of kernels without incurring significant code bloat or runtime overhead, especially for Ascend operator libraries where kernel parameters are too many to be enumerated.

\subsection{Existing Constant Propagation Techniques}

Since neither manual constant propagation nor runtime JIT compilation adequately addresses the challenges of constant specialization in operator libraries, 
it is necessary to design a specialization framework based on static program analysis at the offline compilation stage. Existing constant propagation techniques can be broadly categorized into two primary methodologies:

\begin{itemize}
    \item \textbf{Data Flow Analysis (DFA):} A classic algorithm for constant propagation, DFA models the program as a Control Flow Graph (CFG) and propagates values until a fixed-point is reached. Its theoretical foundation, Abstract Interpretation, ensures soundness and termination by defining abstract value domains (e.g., \textit{Top}, \textit{Constant}, \textit{NAC}) and \texttt{join} operators. While DFA is computationally efficient, it is \textbf{path-insensitive}, leading to a loss of precision at control-flow merge points.

    \item \textbf{Symbolic Execution (SE):} A \textbf{path-sensitive} methodology that explores individual program paths, maintaining variable states as symbolic expressions and tracking branch constraints. SE offers high precision and can identify path-dependent constants missed by DFA. However, it suffers from the \textbf{path explosion} problem, making it computationally expensive and often impractical for this task.
\end{itemize}

However, existing constant propagation techniques are primarily designed with general-purpose programs in mind, and they often fail to meet the stringent requirements of operator libraries, where the number of deduced constant values is critical. The main challenges that hinder their effectiveness in this context include the following aspects:
\begin{itemize}
    \item \textbf{Dynamic Shape Checks:} Operator programs frequently involve dynamic shape checks, resulting in multiple program paths. Accurately identifying and eliminating unreachable paths is essential to maintain precision during constant propagation.
    \item \textbf{Precise Pointer Analysis:} C++ operator programs heavily rely on polymorphic types and pointers. It is crucial to perform precise pointer analysis throughout the constant propagation process, including resolving function pointers and determining which fields of struct objects are being accessed.
    \item \textbf{Handling Uncertain Pointer Targets:} When faced with uncertain pointer targets, traditional methods often mark all potential memory regions as \kw{Unknown}, which can contaminate the analysis and reduce its effectiveness.
\end{itemize}

The above challenges necessitate a specialized constant propagation framework tailored for C++-based operator libraries, which motivates us to develop \mysys as shown in \autoref{sec:design}.
