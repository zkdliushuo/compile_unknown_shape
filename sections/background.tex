\section{Background}
\label{sec:overview}

\subsection{Ascend Architecture and Chanllenge}

\begin{figure}
    \includegraphics[width=\textwidth]{figures/motivation/ascend-data-path.png}
    \caption{Overview of Ascend Architecture. The figure illustrates the key components of the Ascend architecture.}
    \label{fig:ascend_architecture}
\end{figure}

The Ascend architecture as shown in \autoref{fig:ascend_architecture} is designed to efficiently handle a variety of deep learning workloads by incorporating specialized hardware components. The key components of the Ascend architecture include:
\begin{itemize}
    \item \textbf{Computation Units:} The architecture features several computation units, including Scalar Units for control flow processing and instruction dispatching, Vector Units for vectorized operations, and Cube Units for matrix computations. These units work together to execute complex deep learning operations efficiently.
    
    \item \textbf{Memory Hierarchy:} Ascend employs a multi-tiered memory hierarchy with specialized buffers. The L0 A/B/C buffers are dedicated to Cube Units, while the Unified Buffer serves Vector Units. This design allows for efficient data movement between different storage tiers, optimizing memory access patterns for various workloads.
    
    \item \textbf{Data Movement Mechanisms:} The architecture includes flexible data movement mechanisms that facilitate efficient transfer of data between computation units and memory buffers. This is crucial for maintaining high throughput and minimizing latency during deep learning operations.
\end{itemize}

These components work together to provide a programmable and high-performance platform for deep learning applications, enabling developers to optimize their workloads effectively.

While such programmability and optimization potential surpass that of general-purpose GPUs, it introduces significant challenges in kernel optimization. 
Developers must carefully manage parallelism and load balancing through parameters controlling tile sizes, iteration orders, buffer counts, and data movement patterns across memory hierarchies. 
For dynamic-shape models (e.g., Transformers with runtime-variable tensor dimensions), these control parameters, along with shape parameters, ragged tensor offsets, and data formats, become formal arguments of kernel functions. 
At the beginning of kernel execution, these parameters need to be loaded from the global memory of the device to registers and the unified buffer, which then executes control flow instructions and scalar computations dependent on these parameter variables.
This process involves many load and store operations, consuming valuable scalar registers, thus leading to considerable initialization overhead on the Scalar Unit.
In workloads with low computational intensity, the proportion of such initialization overhead becomes significant such as Flash Decoding, whose number of parameters is large and single execution time is small.

We have analyzed that since most kernel parameters are actually compile-time constants when model configurations (e.g., attention heads, embedding sizes) and hardware specifications are fixed, there exists a potential opportunity to accelerate Ascend kernels by identifying these compile-time constants and generating optimized kernels through DSA-native compilation.
However, we find it diffucult to automatically identify compile-time constant parameters for operator programs based on C++ implementation, which we will discuss in the next subsection.

\subsection{Current Kernel Specialization Methods}

Currently, there are two common methods for specializing kernel parameters in DSA operator libraries. The first method involves enumerating certain critical kernel parameters (e.g., tile size) during the offline stage and precompiling the corresponding specialized kernels. The second method determines the parameter values at runtime and performs just-in-time (JIT) compilation based on these values.

\begin{minipage}{0.48\textwidth}
\begin{lstlisting}[language=C++, basicstyle=\tiny, caption={Offline compilation.}]
void matmul_kernel(float* A, float* B, float* C, int M, int N, int K) {
    int TILE_SIZE = calculate_tile_size(M, N, K);
    switch (TILE_SIZE) {
        case 16: matmul_kernel_16(A, B, C, M, N, K); break;
        case 32: matmul_kernel_32(A, B, C, M, N, K); break;
        case 64: matmul_kernel_64(A, B, C, M, N, K); break;
        default: throw std::runtime_error("Unsupported TILE_SIZE");
    }
}
void matmul_kernel_16(float* A, float* B, float* C, int M, int N, int K) {
    constexpr int TILE_SIZE = 16;
    for (int i = 0; i < M; i += TILE_SIZE) {
        for (int j = 0; j < N; j += TILE_SIZE) {}
    }
}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\begin{lstlisting}[language=C++, basicstyle=\tiny, caption={Runtime JIT compilation.}]
void matmul_kernel(float* A, float* B, float* C, int M, int N, int K) {
    int TILE_SIZE = auto_tune_tile_size(M, N, K);
    auto compiled_kernel = jit_compile_kernel(TILE_SIZE);
    compiled_kernel(A, B, C, M, N, K);
}
std::function<void(float*, float*, float*, int, int, int)> 
jit_compile_kernel(int TILE_SIZE) {
    return [TILE_SIZE](float* A, float* B, float* C, int M, int N, int K) {
        for (int i = 0; i < M; i += TILE_SIZE) {
            for (int j = 0; j < N; j += TILE_SIZE) {}
        }
    };
}
\end{lstlisting}
\end{minipage}


