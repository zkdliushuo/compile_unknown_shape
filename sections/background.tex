\section{Background}
\label{sec:overview}
\subsection{Ascend Architecture and Challenges}

\begin{figure}
  \includegraphics[width=0.7\textwidth]{figures/motivation/ascend-data-path.png}
  \caption{Overview of Ascend Architecture. The figure illustrates the key components of the Ascend architecture.}
  \label{fig:ascend_architecture}
\end{figure}


The Ascend architecture, as illustrated in \autoref{fig:ascend_architecture}, is designed to efficiently handle diverse deep learning workloads by incorporating specialized hardware components. Key components include:
\begin{itemize}
    \item \textbf{Computation Units:} The architecture features heterogeneous computation units: \textbf{Scalar Units} for control flow processing, instruction dispatching, and scalar arithmetic; \textbf{Vector Units} for SIMD (Single Instruction, Multiple Data) operations; and \textbf{Cube Units} for high-throughput matrix multiplications. These units collaborate to execute complex neural network operators efficiently.
    
    \item \textbf{Memory Hierarchy:} Ascend employs a multi-tiered memory hierarchy with specialized buffers. The L0 A/B/C buffers are dedicated to Cube Units, while the Unified Buffer serves Vector Units. This design facilitates high-bandwidth data movement between different storage tiers, optimizing memory access patterns.
    
    \item \textbf{Data Movement Mechanisms:} The architecture includes flexible Direct Memory Access (DMA) mechanisms that enable efficient data transfer between computation units and memory buffers. This is crucial for maintaining high pipeline utilization and minimizing latency.
\end{itemize}

While these components provide a highly programmable and high-performance platform, they introduce significant challenges in kernel optimization compared to general-purpose GPUs. 
Ascend developers must meticulously manage parallelism and load balancing through a vast set of parameters controlling tile sizes, iteration orders, buffer counts, and data movement patterns across the memory hierarchy. 
For dynamic-shape models (e.g., Transformers with runtime-variable sequence lengths), these scheduling parameters, along with tensor shapes and offsets, are passed as formal arguments to kernel functions. 

At the start of kernel execution, these parameters must be loaded from global memory into registers or the Unified Buffer. The Scalar Unit then executes control flow instructions and scalar computations dependent on these variables. 
Since the Scalar Unit typically has a limited number of general-purpose registers, processing a large number of dynamic parameters triggers frequent load/store operations and register spilling. 
This results in a \textbf{Scalar Bottleneck}, where the latency of the Scalar Unit limits the overall throughput. 
In workloads with low computational intensity but high scheduling complexity, such as Flash Decoding, this scalar overhead consumes a significant portion of the execution time.

We observe that while input shapes vary dynamically, many kernel parameters effectively become \textit{compile-time constants} once model configurations (e.g., attention heads, embedding sizes) and hardware specifications are fixed. 
This reveals an opportunity to accelerate Ascend kernels by identifying these constants and generating specialized kernels via the native compiler. 
However, identifying these constants automatically from C++ based operator libraries presents unique challenges, discussed next.

\subsection{Current Kernel Specialization Methods}

There are two primary methods are currently employed for specializing kernel parameters in other DSA operator libraries(e.g., GPU): Ahead-of-Time (AoT) compilation and Just-in-Time (JIT) compilation.
\autoref{lst:compare_aot_jit_without_graph} demonstrates specializing the tile size parameter \kw{T} in a simple addition kernel using both methods.

\begin{figure}[htbp]
\begin{minipage}{0.39\textwidth}
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, caption={Templated kernel for kernel \kw{add}}, captionpos=b, frame=single]
template <int T>
void kernel(float* A, float* B, float* C, int s) {
  for (int i = 0; i < s; i += T) {
    for (int j = 0; j < T; ++j) {
      int idx = i + j;
      if (idx < s)
        C[idx] = A[idx] + B[idx];
    }
  }
}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{0.56\textwidth}
\begin{lstlisting}[firstnumber=11, basicstyle=\ttfamily\footnotesize, caption={AoT and JIT implementations of operator \kw{add}}, captionpos=b, frame=single]
status aot_add(Tensor& A, Tensor& B, Tensor& C) {
  int t = 0; tiling(A.size(0), t);
  switch (t) {
    case 16: kernel<16>(..., A.size(0)); break;
    case 32: kernel<32>(..., A.size(0)); break;
}}
status jit_add(Tensor& A, Tensor& B, Tensor& C) {
  int t = 0; tiling(A.size(0), t);
  auto kernel_ = run_compile(kernel, t);
  kernel_(A.ptr, B.ptr, C.ptr, A.size(0));
}
\end{lstlisting}
\end{minipage}
\caption{Comparison of AoT compilation and JIT compilation for kernel specialization of add operator.\scnote{\kw{aot\_add} returns status but \kw{jit\_add} returns void}}
\label{lst:compare_aot_jit_without_graph}
\end{figure}

\textbf{AoT Compilation:} As shown in \kw{aot\_add}, developers manually precompile kernels for a predefined set of parameter values, enumerated using a \kw{switch-case} statement. 
This approach enjoys little runtime overhead but is severely limited by the \textit{code bloat} problem. 
With operator libraries often containing dozens of parameters, exhaustively enumerating all possible combinations becomes infeasible.
As a result, AoT methods can only specialize a small subset of parameters.
Moreover, for a specific model deployment, most parameters are fixed, meaning only a tiny fraction of these precompiled kernels are ever utilized, while the rest remain redundant.

\textbf{JIT Compilation:} As shown in \kw{jit\_add}, the parameter value \kw{t} is determined at runtime, and a specialized kernel \kw{kernel\_} is compiled for the specific \kw{t}. 
This approach provides maximum flexibility but incurs significant overhead due to runtime compilation and storage requirements. 
For latency-sensitive inference scenarios, this overhead is often prohibitive, even when caching mechanisms are employed to reuse compilation results.

The key observation is that once model hyperparameters and hardware configurations are fixed, most kernel parameters can be treated as compile-time constants. 
Existing AoT and JIT methods attempt to specialize a small subset of these parameters but fail to fully exploit this insight due to their inherent limitations. 
To overcome these challenges, it is essential to design a constant specialization framework capable of automatically identifying compile-time constants based on the specific model context and generating specialized kernels offline, thereby avoiding both code bloat and runtime overhead.

\subsection{Existing Constant Propagation Techniques}

To achieve automatic specialization, one must rely on static program analysis to deduce constant values. Existing constant propagation techniques can be broadly categorized into two methodologies:

\begin{itemize}
    \item \textbf{Data Flow Analysis (DFA):} 
    DFA is a classic algorithm grounded in the theory of Abstract Interpretation. It models the program as a Control Flow Graph (CFG) and propagates abstract values (e.g., lattice elements like \textit{Top}, \textit{Constant}, \textit{Bottom}) iteratively until a fixed point is reached. 
    DFA offers a good balance between precision and efficiency. 
    However, standard DFA is typically \textbf{path-insensitive}. When control flows merge (e.g., after an \kw{if-else} block), DFA merges the abstract states from all incoming paths using a \kw{join} operator. This often results in a loss of precision, degrading a constant value to \textit{Unknown} (Top) if branches disagree.

    \item \textbf{Symbolic Execution (SE):} 
    SE explores individual program paths, maintaining the program state as symbolic expressions and tracking path constraints. 
    It is \textbf{path-sensitive}, distinguishing values across different execution paths, which offers higher precision than DFA. 
    However, SE suffers from the \textbf{path explosion} problem. As the number of branches increases, the number of paths grows exponentially, making SE computationally prohibitive for analyzing large-scale C++ operator libraries.
\end{itemize}

These techniques are mature for single-language analysis (e.g., analyzing pure C++ or LLVM IR). However, applying them to Deep Learning (DL) frameworks introduces a novel \textbf{Cross-Language Analysis} challenge. 
DL frameworks typically use Python for the frontend and C++ for the backend operator library. 
There exists a significant "semantic gap" between the Python runtime (where constant shape values are known) and the C++ static analysis environment (where the kernel code resides).

To the best of our knowledge, existing cross-language analysis tools primarily focus on type checking, security, or interface binding generation. 
There is a lack of work on \textbf{propagating concrete runtime values from a high-level dynamic language (Python) to the low-level binary layout of a static language (C++)} to enable bit-level constant propagation for compiler optimizations.
Specifically, two main obstacles hinder the direct application of existing DFA or SE:

\begin{enumerate}
    \item \textbf{Semantic opaque across language boundaries:} 
    The constant information (e.g., tensor shapes) is available in the Python runtime context. However, when analyzing the C++ operator binary (or IR), this information is lost. The analyzer sees only opaque pointers to C++ objects (like \kw{torch::Tensor}) without knowing the memory layout or the specific values stored within. This makes it impossible to initialize the analysis context correctly using only C++ static analysis tools.
    
    \item \textbf{Precision loss from dynamic shape checks:} 
    Operator libraries contain extensive defensive checks for dynamic shapes (e.g., \kw{if (shape \% 32 != 0) return error;}). 
    Since some dimensions remain symbolic during partial evaluation, a path-insensitive DFA cannot determine which branch is taken. It conservatively merges the "error path" (where values are undefined) with the "valid path," causing constant values to degrade to \textit{Unknown}. While SE could theoretically handle this, the path explosion from numerous checks makes it impractical.
\end{enumerate}

The above challenges motivate us to develop \mysys, as described in \autoref{sec:design}, which addresses the cross-language gap via context reconstruction and enhances precision through pattern-based pruning.

%sxy% 第二章Background和第三章challenge章节，看起来内容非常充实并且有很多例子。但是结构组织上让我有点困惑：例如第二章Background中似乎也在分析challenge；而第三章challenge中还介绍了本文克服challenge的Approach。
%sxy% 我建议更清晰的组织章节：在第二章Background中只介绍背景知识，在第三章challenge中集中分析挑战，在后续介绍Approach的章节中再介绍本文是如何克服challenge的。另一种改法是将Background和challenge两个章节合并为一个章节，不过这个章节可能会过长。