\section{Introduction}\label{sec:intro}

随着深度学习的发展，许多大型模型不断涌现并在众多任务中表现出色，如 Llama 3、DeepSeek、Qwen 和 OpenSora等等。
为了加速大模型的训练和推理，更好地提供庞大的算力支撑，许多公司开发了各自的领域专用架构（DSA），包括英伟达GPU、华为 Ascend NPU、谷歌 TPU 和寒武纪 MLU 等。
作为这些芯片的代表之一，Ascend NPU 目前已经广泛支持了各类有影响力的模型，例如Qwen、DeepSeek系列的模型，Ascend 取得了高性能和低功耗。

相较于通用 GPU，Ascend 的特点是额外引入了几种可编程的硬件部件，以高效支撑不同类型的计算：
1）计算部件，包括处理控制流、执行指令分发和标量数据计算的 Scalar unit，分别处理向量化和矩阵操作的 Vector 和 Cube Unit。
2）多种memory buffer和数据搬运部件：包括面向 Cube Unit 的 L0 A/B/C buffer，以及面向 Vector Unit 的 Unified buffer。这些存储部件之间的数据搬运方式也很灵活，
这些硬件部件之间的数据通路我们将在后文中详细介绍，参见xxx.

上述的设计给开发人员提供了超越 GPU 的可编程性和性能优化潜力，
但这也导致 Ascend 核函数的开发者需要更仔细地控制对性能十分重要的并行度以及负载均衡，
开发者通常会为算子内的操作定义一组参数，用于控制核函数的每个层级的存储层级上 macro-kernel 的 tile size、迭代顺序、缓冲区数目，以及各个存储层级之间的数据搬运方式。
为了支持诸如 Transformer 架构的动态形状模型（输入张量的形状是动态、运行时可变的），上述这组控制参数，连同算子的形状参数、ragged tensor(batch 内 tensor 形状不同)的 offsets 数组、输入输出的数据格式等一起，被实现核函数的形式参数。
这些核函数参数需要运行时根据输入形状即时确定再从 Host 传递到 Device，这导致 Scalar Unit 需要实际执行依赖这些参数变量的控制流指令、标量计算并消耗宝贵的通用寄存器（类似于 CPU，每个Davinci Core 核心只有 32 个通用 64 bit 寄存器）存储中间结果。
在一类工作负载降低的核函数中，由于 Cube、 Vector 以及数据搬运类的操作次数较少，难以有效掩藏 Scalar Unit 上的任务。
尤其是诸如 Flash Decoding （用于 LLM 模型自回归解码阶段执行高效注意力计算）这类工作负载较低、计算逻辑又相对复杂的算子，
Scalar Unit bound 实际上是这类核函数的一个关键性能瓶颈。

我们统计了 CANNDev、CANN OPS ADV 两个在 Ascend 设备上最广泛使用的算子库的部分算子的各类硬件部件的利用率。
图中纵轴代表算子执行中，一种硬件部件的 active 周期数目。
注意，由于 SIMD 架构的 DSA 多个硬件部件通常并行执行，所有硬件部件的总周期数目实际上大于算子的总执行周期数目。
上图证明 Ascend 的核函数性能对核函数的参数数目敏感，过多的参数可能会导致标量部件成为瓶颈，其他运算和访存部件由于等待标量部件而处于 idle 状态。
进一步地，我们分析发现，固定模型参数（例如注意力头数目、embedding size、最大输入长度等）和硬件配置后，大部分核函数参数都是编译期常量（运行时为不变量且可以在编译期静态求值）。
如表 xxx 所示，这揭示了潜在加速 Ascend 核函数的机会：识别属于编译期常量的核函数参数在编译期替换为常量值，
并通过 DSA native 编译器编译产生针对特定模型和硬件的特化优化的核函数。

据我们所知，当前从 DSA 算子库提取核函数可以被特化的参数的方法主要有两种：
1）对特定的核函数参数执行手工的常量传播；2）基于自动调优 (AutoTuning) 或即时编译确定部分核函数参数。
手工常量传播通常只对少数特定的核函数参数做特化，这种方法预先枚举每个参数的所有可能取值并为每种取值特化生成一个核函数。
这种方式通常只适用于核函数参数较少、手工枚举待特化参数的所有取值不是特别复杂的情况，
由于 GPU 的 SIMT 架构的可编程硬件较少，算子库核函数的参数数目也较少，因而采取了这种方法。
但手工常量特化的不足在于可能会导致严重的代码膨胀问题。
出于泛化性以及减少运行时编译开销的考虑，算子库通常采取 AOT 编译方式，编译前需要适配好各类模型和各种硬件，导致代码和编译产物的基础体积就已经很大了。
因而即便是只枚举部分核函数参数的少量取值，也通常会使代码体积严重增加。
一些 AI Compiler 或者高层张量编程 DSL (Linalg, Triton, etc.) 的算子实现主要通过自动调优 (AutoTuning) 以及即时编译的方式实现自动常量特化核函数的部分参数。
但这种常量特化严重依赖根据运行时形状实际执行 wrapper code 并启用在线常量特化，带来 CPU 上的执行和编译开销。
因而在模型工业部署落地场景中，当前依然以手工算子库为主，AI Compiler 自动生成的算子为辅的方式提供高性能推理服务。

综上原因，我们认为有必要设计一套常量特化框架，实现根据算子的输入形状中的常量维度的取值和算子的 Host code，自动识别出核函数参数中哪些参数是编译时常量，从而实现对核函数的自动常量特化。

针对算子库通常基于 C++ 开发这一事实，本文提出一个基于跨过程常量传播实现的算子自动常量特化方法。
我们的方法可以用于降低算子的推理时延，尤其是降低算子的标量计算开销，也能实现对算子库的代码瘦身，后者在资源受限的端侧设备以及无服务计算上更重要。


在编译时，我们基于深度学习框架（例如 PyTorch Dynamo），运行动态形状模型，捕获得到每个算子的每个输入输出的 symbolic shape 以及算子属性（例如注意力头数目、embedding size）的常量值。
此外，我们手动为硬件构造了一份配置文件，描述了 Host Code 可能会查询的硬件基本信息，例如计算核心数目以及 L1/L2 Cache 的大小等信息。
对于整个算子库，我们将其中的 Host 程序降低到 LLVM IR，我们为每个算子的 Host Code 的入口函数根据前面获取的输入输出形状和算子属性信息，构造一份常量特化的上下文。
基于算子程序 LLVM IR 和常量特化上下文，我们提出一种基于抽象解释的常量传播框架，称作 \mysys 用于推导出核函数的编译期常量的参数值。
动态形状检查在算子程序中普遍存在，针对变量维度及其表达式的条件检查语句会造成多条程序路径，为此我们提出一种识别不可达路径的方法来避免分析精度的降低。
此外，针对 C++ 算子程序中普遍使用的多态类型以及指针，为提升分析精度，我们在抽象解释中引入了指针指向的传播。
此外，当遇到不确定的指针指向时，不同于传统的方法为了保证分析的正确需要将所有可能指向的内存区域都设置为 Unknown，这可能会导致我们的分析对象被污染，我们采取了相当激进的实现：忽略不确定的指针指向，不更新任何内存，我们将正确性的检查留给运行时的正确性检测。
通过上述常量传播，我们可以分析得出核函数的部分参数取值，并将这些参数替换为对应的常量，再对核函数进行常量相关的优化，得到特化的核函数程序。
在运行时，对于给定的运行时形状，我们首先运行一遍 Host Code 得到核函数的全部参数的值。
我们首先将这些参数与特化的核函数程序的参数值做对比，若相同则调用特化核函数；否则，调用原版非特化的核函数。


Deep learning systems organize computation into graphs of tensor operators. A key challenge is that tensor shapes are often unknown during graph construction, particularly in models that support dynamic input sizes. This shape variability directly impacts how operators are implemented on hardware.

% 不需要强调CPU计算 tiling 参数引起的标量计算开销，而是重点强调核函数处理传递进来的 tiling 参数的标量计算开销 
Whether generated by compilers or crafted by experts, operator implementations typically translate \kw{dynamic tensor shapes} into \kw{hardware-aware tiling parameters}.
These parameters, derived at runtime based on input tensor shapes $\overrightarrow{S}$ and hardware characteristics $H$, set up execution strategies (\eg tile size and address offsets) inside the kernel through \textbf{additional scalar computations}, enabling a single kernel to support varying input sizes.

For example, in matrix multiplication (MatMul) (shown in \autoref{fig:matmul-definition}), the CPU-side host code computes tiling parameters 
$\overrightarrow{p}=f(\overrightarrow{S},H)$, where $f$ is a tiling function that maps tensor shapes and hardware characteristics to efficient tiling configurations. 
These parameters are then passed to the device kernel: $\mmkernel(\texttt{A,B,C,}\overrightarrow{p})$, where $A$ and $B$ are input matrices and $C$ is the output. 
However, excessive reliance on runtime parameters can limit compiler optimizations and introduce a critical performance issue known as the \textbf{scalar bottleneck}.

On Nvidia GPUs, for example, when tiling parameters $\overrightarrow{p_M}$ ($\overrightarrow{p_M} \subset \vec{p}$), which govern control flow within the kernel, are treated as runtime variables, the compiler cannot apply optimizations like loop unrolling or instruction scheduling, leading to inefficient kernels.

\begin{figure}[t]
    \begin{subfigure}[b]{\linewidth}
        \vspace{0pt}
        \includegraphics[width=1\linewidth]{figures/motivation/pipeline-dynamic.png}
        \caption{Unspecialized kernel with runtime tiling parameter computation}%Execution pipeline of different processing units of a unspecialized $\mmkernel$ kernel for any possible input shapes in \autoref{fig:matmul-definition}}
        \label{fig:dynamic-pipeline}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{\linewidth}
        \vspace{0pt}
        \includegraphics[width=1\linewidth]{figures/motivation/pipeline-static.png}
        \caption{Specialized kernel with compile-time constant tiling parameters}%Execution pipeline of different processing units of a specialized $\mmkernel$ kernel for a static input shape in \autoref{fig:matmul-total-specialization}}
        \label{fig:static-pipeline}
    \end{subfigure}
    \caption{Execution pipeline comparison of MatMul kernels on a commercial NPU with Scalar, Cube, and HBM (high bandwidth memory) units running in parallel. The scalar unit in (a) becomes a performance bottleneck due to excessive runtime computations. \textit{Vector} units are omitted for simplicity.}
    \label{fig:pipeline}
\end{figure}

On NPUs such as Ascend~\cite{liao2019davinci} and other specialized hardware, excessive runtime parameters may overload the \textit{Scalar} units responsible for instruction dispatch and scalar calculation.
\autoref{fig:pipeline} shows the execution pipelines of MatMul on a commercial NPU with Scalar units, high-bandwidth memory (HBM), and Cube units. Subfigure (a) uses the original kernel where all tiling parameters are computed at runtime, \ie %$\mmkernel(\texttt{A,B,C,}\overrightarrow{p})$ 
$\kDef{MM}{A,B,C}{p}$ in \autoref{fig:matmul-definition}. Subfigure (b) uses a specialized kernel where all tiling parameters are known before execution, \ie %$\instantiated{\hlight{\overrightarrow{c_1}}}\text{A,B,C})$
$\kSpecial{MM}{A,B,C}{c_1}$ in \autoref{fig:matmul-total-specialization}. 
%this effect with MatMul on a NPU, comparing a fully specialized kernel $\instantiated{\hlight{\overrightarrow{c_1}}}(\text{A,B,C})$(\autoref{fig:matmul-total-specialization}) against the original kernel(\autoref{fig:matmul-definition}). 
%The comparison reveals that performance degradation of the unspecialized kernel is caused by two primary factors.
Kernel (a) is slower than (b) for two primary factors.
First, scalar computation overhead prolongs the initialization phase (1), as many values must be computed at runtime instead of being constant-folded.
Second, the use of many runtime variables increases control-flow complexity, leading to register spilling and ICache misses in phases (2) and (3). 
Together, these result in a 34\% increase in total execution time.

\iffalse
For example, the MatMul operator defined in \autoref{fig:matmul-definition} consists of two parts: \textbf{host code} $f$ that runs on the CPU and the
\textbf{kernel function} \mmkernel that executes on an accelerator device like a GPU or NPU.
The host code is responsible for calculating a list of tiling parameters $\overrightarrow{p}$ based on runtime information, such as the input tensor shapes $\overrightarrow{S}$ and specific hardware resource information $H$. 
These parameters $\overrightarrow{p}$ are then passed to the kernel function for execution. 

However, too many parameters may hurt kernel performance on AI accelerators, leading to high demand of \texttt{constant specialization}: 
resolving kernel parameters at compile-time to generate specialized code.
\fi

\begin{figure*}[ht]
    \centering
    \hfill
    % (a) 子图一：完全动态
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \begin{lstlisting}
A$\in\sR^{m\times k}$,B$\in\mathbb{R}^{k \times n}$
C$\in\mathbb{R}^{m\times n}$
$\vec{S_D} = (m, n, k)$
$\vec{p} = f(\vec{S_D}, H)$
$\kDef{MM}{A,B,C}{p}$
        \end{lstlisting}
        % --- 【修改处】 ---
        \caption{Fully dynamic, \\ no specialization}
        \label{fig:matmul-definition}
    \end{subfigure}
    \hfill
    % (a) 子图二：完全静态
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \begin{lstlisting}
A$\in\sR^{\hlight{96} \times \hlight{32}}$,B$\in\sR^{\hlight{32} \times \hlight{64}}$
C$\in\sR^{\hlight{92} \times \hlight{64}}$
$\vecC{c_0} = (\hlight{92},\hlight{32},\hlight{64})$
$\vecC{c_1} = f(\vecC{c_0}, H)$
$\kSpecial{MM}{A,B,C}{c_1}$
        \end{lstlisting}
        % --- 【修改处】 ---
        \caption{Fully static, \\ Just-In-Time specialization}
        \label{fig:matmul-total-specialization}
    \end{subfigure}
    \hfill
    % (b) 子图三：手动特化
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \begin{lstlisting}
$\vec{p} = f(\vec{S_D}, H)$
$\vec{p_M}, \vec{p_A} = \texttt{partition}(\overrightarrow{p})$
switch($\overrightarrow{p_M}$):
 case $\vecC{c_2}$: $\kSpecialD{MM}{A,B,C}{c_2}{p_A}$
 case $\vecC{c_3}$: $\kSpecialD{MM}{A,B,C}{c_3}{p_A}$
 case ...
        \end{lstlisting}
        % --- 【修改处】 ---  
        \caption{Fully dynamic, \\ manual specialization}
        \label{fig:manual-dispatched-invocation}
    \end{subfigure}
    \hfill
    % (c) 子图四：自动特化
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \begin{lstlisting}
A$\in\sR^{m\times \hlight{32}}$,B$\in\sR^{\hlight{32} \times \hlight{64}}$
C$\in\sR^{m\times \hlight{64}}$
$\vec{S_P} = (m, \hlight{64}, \hlight{32})$ 
$\vec{p_C}, \vec{p_D} = f_2(\vec{S_P}, H)$
assert $\vec{p_C} = \vecC{c_4}$
$\kSpecialD{MM}{A,B,C}{c_4}{p_D}$
        \end{lstlisting}
        \caption{Partial dynamic, \\ automatic specialization}
        \label{fig:compiler-specialization}
    \end{subfigure}
\caption{Four different implementations of MatMul, where notations satisfy: $\overrightarrow{p} = \overrightarrow{p_A} \cup \overrightarrow{p_M} = \overrightarrow{p_C} \cup \overrightarrow{p_D}$. For simplicity, we omit the launching configuration parameters such as grid size and thread    block size.
}
\label{fig:specialization-approaches}
\end{figure*}

This motivates \kw{constant specialization}, where parameters are resolved ahead of time to generate specialized, high-performance kernels.
To our best knowledge, there are two primary strategies.
The first is Just-In-Time (JIT) compilation (\autoref{fig:matmul-total-specialization}), which compiles a specialized kernel at runtime based on actual input shapes, as used in systems like Dynamo~\cite{ansel2024dynamo}, CANN~\cite{HuaweiCANN}, and Triton~\cite{tillet2019triton}. 
While effective when shapes are fully static, this approach is impractical for dynamic scenarios due to the cost of compiling kernels for every possible shape. 
The second strategy is \kw{manual dispatch} (\autoref{fig:manual-dispatched-invocation}), a pre-compilation approach widely adopted by libraries like CUTLASS and FlashInfer \cite{nvidia2017cutlass,ye2025flashinfer}.
%\autoref{fig:manual-dispatched-invocation} shows an example of \kw{manual dispatch} where 
In this method, the kernel template $\mmkernel$ is instantiated multiple times by specializing the tiling parameter subset $\vec{p_M}$ with enumerated constant lists (\eg $\vecC{c_2}$, $\vecC{c_3}$), each corresponding to a concrete parameter configuration.
%seperately using constant lists $\overrightarrow{c}_1,\overrightarrow{c}_2,\dots$, which are possible values of parameters $\overrightarrow{p_M}$.

Despite its wide adoption, the manual dispatch approach suffers from two major limitations:
\begin{itemize}
    \item \textbf{Insufficient specialization:} To handle diverse dynamic shapes, complex kernels (\eg Flash Attention~\cite{dao2024flashattention2}, Paged Attention~\cite{kwon2023vllm}) rely on numerous kernel parameters.
    Manually enumerating all of them for specialization is infeasible,
    leaving many parameters to be resolved at runtime ($|\vec{p}| \gg |\vec{p_M}|$), and missing performance opportunities.
    \item \textbf{Code size bloat:} Even specializing a small subset of tiling parameters can trigger a combinatorial explosion in kernel versions. This leads to an inflate binary size and longer cold start time, a critical drawback for serverless deployment~\cite{zhang2025the}.
\end{itemize}

\begin{figure}[b]
    \centering
    %\begin{minipage}[t]{0.95\linewidth}
    \centering
        \includegraphics[width=0.8\linewidth]{figures/intro/intro2-partial-dynamic-op-ours.png} 
        \caption{Specialization pipeline of \mysys{}. %Dashed arrows show the \textbf{compile-time workflow} and the solid arrows show the \textbf{runtime data flow}. 
The actual arguments are set blue(\eg \textcolor{blue}{$\overrightarrow{p_D}$}) for identification.}
        \label{fig:this-work-overflow}
    %\end{minipage}
\end{figure}

To address these limitations, we propose \textbf{compiler-driven automatic kernel specialization}.
The key insight is that many tiling parameters are constant for a specific model, as they depend only on fixed dimensions (\eg attention head number).
As shown in \autoref{fig:this-work-overflow}, the compiler is provided with partially known shape dimensions such as ($m$, \hlight{64, 32}) and then analyzes the host code to determine which kernel parameters are compile-time invariants. It then computes their concrete values (\eg $\vecC{c_4}$) and automatically transforms the operator program to invoke a specialized kernel, such as $\kSpecialD{MM}{A,B,C}{c_4}{p_D}$( \autoref{fig:compiler-specialization}).  At runtime, the remaining tiling parameters $\vec{p_D}$ are derived from the actual input shapes and hardware configuration $H$ and passed--along with concrete matrices--into the specialized kernel $\kSpecialNm{MM}{c_4}$ to complete execution. This fully automated process eliminates the need for manual enumeration, avoiding both insufficient specialization and code bloat.

\begin{table}[t]
\centering
\caption{Notations of this paper}
\label{tab:symbol-definitions}
\small
\begin{tabularx}{0.9\linewidth}{r @{ -- } >{\raggedright\arraybackslash}p{3cm} @{\hspace{0.2em}} r @{ -- } >{\raggedright\arraybackslash}X}
    \hline
    $m$,$n$,$k$      & Symbolic dimensions        & $\vec{p_M}$ & Manual constants   \\
    $\vec{S_D}$  & Fully dynamic shape      & $\vec{p_A}$            & Manual variables   \\
    $\vec{S_P}$  & Partial static shape     & $\vec{p_C}$            & Inferred constants \\
    $H$          & Hardware information             & $\vec{p_D}$            & Inferred variables \\
    $\vecC{c}$  & List of constants     & $\vec{p}$    & Tiling parameters       \\
    \multicolumn{4}{c}{
        $\kSpecial{MM}{\dots}{c}$ -- Kernel $\mmkernel$ instantiated with constant list $\vecC{c}$
    } \\
    \hline
\end{tabularx}
\end{table}
% In this paradigm, the compiler automatically analyzes the host code to determine which kernel parameters are compile-time invariants,
% which we denote as $\overrightarrow{p_C}$ and their specific compile-time values, denoted as $\overrightarrow{c}$. 
% With this information, the compiler can then automatically transform the code to invoke a kernel specialized with these constants, $\mmkernel\langle\overrightarrow{c}\rangle$, eliminating the need for manual enumeration and its associated drawbacks.

% 核函数的自动常量特化要求采用精确和高效的 IPCP 流程来
Automatic kernel specialization requires a precise and efficient inter-procedural constant propagation (IPCP) pipeline.
However, operator programs, often containing up to 10k lines of C++ code, poses two key challenges:
\begin{itemize}
\item \textbf{Early return caused by runtime shape checks:}
Operator code often include shape validations that exit early on invalid inputs,
meaning constants only exist along valid paths. 
Standard program analysis merges states from all control paths, which obscures these constants and significantly reduces propagation accuracy.
\item \textbf{Imprecise point-to analysis for non-constant pointers:}
When pointers are not statically resolved, memory writes must conservatively affect many locations. This leads to widespread unknowns. 
Auxiliary point-to analysis \cite{anderson1994program} may help narrow candidate memory locations, but remains imprecise in practice. 
\end{itemize}

%\yinote{and three are to much bold and textit text, should we reduce some highlight?}
To address these challenges, we propose \mysys{}, a tool for automatic operator kernel specialization based on LLVM IR~\cite{lattner2004llvm}. \mysys provides a novel high-precision, low-overhead constant propagation strategy. % that achieves high precision efficiently. 
To mitigate precision loss from early returns, it first performs an inter-procedural analysis pass to identify and remark early-return paths triggered by shape checks. 
Then, during constant propagation, these paths are selectively ignored to preserve constant information. %\mysys{} selectively overlooks the remarked early return paths.
Simultaneously, to handle pointer indirection, \mysys tracks pointer access paths, enabling more precise alias analysis and drastically narrowing memory side effects. These techniques allow \mysys{} to infer constants precisely without resorting to costly path-sensitive analysis.

In summary, the main contributions of this paper are:
\begin{itemize}
    \item We identify that %in modern operator kernels for dynamic shapes, 
    many tiling-related kernel parameters are actually constant for a given model, despite dynamic nature of input shapes.
    This insight reveals an optimization opportunity to solve the scalar bottleneck through automatic specialization.

    \item We propose \mysys{}, an automated framework to analyze operator programs and specialize their kernels by analyzing LLVM IR.
    By effectively handling analysis challenges like early returns from shape checks and pointer aliasing, our framework can efficiently infer constant kernel parameters with high precision.

    \item We demonstrate the effectiveness of \mysys{} on state-of-the-art (SOTA) Attention libraries. 
    On an NPU, our method achieves a 1.11x average speedup over the SOTA baseline. 
    On GPUs, it boosts the performance of manually-specialized libraries by 1.01x to 1.03x while also reducingbinary size by up to 96.2\%.
\end{itemize}


% 我们的方法的输入是从支持捕获 symbolic dimensions 的深度学习框架（PyTorch Dynamo）导出的算子信息，包含输入输出张量和算子属性值。每个输入输出张量表示为 (data\_ptr, shape), 其中 data\_ptr 是运行时数据的指针，此处只起到占位符的作用，shape 是 （）
